{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lnomkin/Text_Analysis_Final_Project/blob/main/311_Data_Comparison_NYC_Open_Data_and_Reddit%2C_de_Blasio_to_Adams_cleared_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#311 Service Request Comparison: NYC Open Data and NYC/311 Subreddit 2020-2023, from the De Blasio to the Adams administrations"
      ],
      "metadata": {
        "id": "0pjKPoEZPjHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Introduction: Understanding the public’s response to social service programming\n",
        "\n",
        "  The pandemic exposed New York City’s deep inequalities, impacting job loss, education access, and homelessness. Today, economic challenges and the end of Covid-19 aid continue to affect low-income communities. The city’s recovery efforts varied under de Blasio and Adams.\n",
        "  \n",
        "  De Blasio, elected on a progressive platform, saw a 15% wage increase for the bottom earners and a 13% decrease in poverty, the lowest since 2005 [(Family, 2022)](https://www.icph.org/reports/family-homelessness-in-new-york-city-what-the-adams-administration-can-learn-from-previous-mayoralties/#keeping-new-yorkers-housed-homelessness-prevention-vouchers-and-housing).  His key achievements included universal Pre-K and 3-K, easing childcare burdens, and streamlining social services. These gains reversed in the pandemic’s first year.\n",
        "  \n",
        "  Adams, elected on a moderate platform, focused on crime, economic recovery, and supporting small businesses [(Get Stuff Done, 2024)](https://www.nyc.gov/content/getstuffdone/pages/initiatives). Under his administration, cash-assistance recipients rose by 23%, while staffing shortages and an asylum crisis strained services [(Family, 2022)](https://www.icph.org/reports/family-homelessness-in-new-york-city-what-the-adams-administration-can-learn-from-previous-mayoralties/#keeping-new-yorkers-housed-homelessness-prevention-vouchers-and-housing). Poverty and unemployment remain high, and Adams faces the lowest approval ratings of any Mayor.\n",
        "  \n",
        "  Analyzing 311 service requests for social services during the pandemic offers insights into residents’ experiences. While Open Data NYC provides request types, the full text is unavailable. Complementing this with sentiment analysis of Reddit discussions can reveal how young adults and older residents perceived government services during this time, the majority of Reddit users [(Anderson, 2024)](https://www.socialchamp.io/blog/reddit-demographics/).\n",
        "\n",
        "  This research is critical for understanding how New Yorkers engage with social services. As the city faces ongoing challenges like the housing crisis, child care shortages, and rising welfare requests, this study can inform policymakers on public sentiment and service effectiveness by answering the following questions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PznULbPiP-Ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.2 Research Questions:\n",
        "1.  Is there a change in sentiment in NYC/311 Reddit posts across the de Blasio and Adams administrations, during a period of a shift in focus from self sufficiency to expanded benefits access?\n",
        "2.  Comparing key social service topics, what are the general trends in 311 service requests and NYC/311 Reddit posts related to social services across the two administrations? What is the sentiment of these themes?\n",
        "3.  What are the most frequently used keywords by the public on the r/NYC 311 subreddit and 311 service complaints?\n"
      ],
      "metadata": {
        "id": "qk16btDQR0S5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.3 Hypothesis:\n",
        "1.  There will likely be more complaints related to welfare and human services during the Adams administration due to expanded access leading to staffing shortages and delays in processing times.\n",
        "2.  The expanded pandemic-related aid created a dependency on social services, with complaints likely focusing on barriers to access, benefit expirations, and challenges in meeting basic needs.\n",
        "3.  Sentiment around social services during the Adams administration is expected to be more negative due to budget cuts, the city's response to the immigration crisis, and efforts to roll back the “Right to Shelter” policy.\n",
        "4.  Frequent keywords under de Blasio will include unemployment, benefits enrollment, and education, while under Adams, they are expected to focus on housing, migrants, and enrollment delays.\n"
      ],
      "metadata": {
        "id": "lOZd9vpCSF2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.4 Significance and Importance to Public Policy:\n",
        "This project offers insights into how New Yorkers respond to social services given mayoral changes. Policymakers could use these findings to assess what areas may need further resources or adjustments. Insights into the sentiment trends could inform perception of policy."
      ],
      "metadata": {
        "id": "j7lGuSPDSP4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Research Methods"
      ],
      "metadata": {
        "id": "EkaHkvNYlqtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1 Method of Data Collection\n",
        "\n",
        "To conduct this analysis, I used the Reddit API to scrape data from the r/nyc/311 subreddit, focusing on posts from 2020-2023, which span the de Blasio and Adams administrations.  I fetched data from the subreddits with mention of 311, including /nyc, /AskNYC, /newyorkcity, /NYChousing, and /311. I limited the time frame of my search to January 1, 2020 - December 31, 2023, gathering two years of data for each administration.\n",
        "\n",
        "I used the Python Reddit API Wrapper (PRAW) which is open to the public. To scrape the date, I created a Reddit account following Reddit’s API Documentation (Reddit Dev Api) then created a personal use script to pull the data. Reddit imposes rate limits of 100 requests per limit, which made it challenging to test my code and pull the data. This also limited my capacity to expand my searches.\n",
        "\n",
        "To compare the Reddit data to citywide 311 service requests, I downloaded 311 Service Request Data from Open Data NYC, a public database of data generated by various NYC agencies and other City organizations. Before downloading the csv file, I queried the data on the website to extract from January 1, 2020 - December 31, 2023 and reduced the columns from 41 down to seven. I had previously attempted to scrape the data through Open Data’s API for Python, but was unable to import it due to the volume of rows. I also had to narrow my search due to data size.\n"
      ],
      "metadata": {
        "id": "OczQe8qcl3lo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 Data Dictionary\n",
        "\n",
        "1.  Subreddit: a forum dedicated to a specific topic, such as ‘311’\n",
        "2.  Title: a submission in a subreddit\n",
        "3.  Parent comment: the top comment of the thread\n",
        "4.  Self text: a post without a link\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UDurcoAXmJbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3 Method of Data Analysis"
      ],
      "metadata": {
        "id": "h6itp5iPmfmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.4 Reddit Analysis\n",
        "I analyzed Reddit post trends by converting data into a dataframe, cleaning it (removing stopwords, tokenizing, lemmatizing), and extracting the top 20 most frequently used words across subreddits to identify policy linkages. Key terms analyzed included “housing,” “benefits,” “shelter,” “wait,” “delays,” “budget,” “immigration,” “migrant,” “covid,” “blasio,” “adams,” “wage,” “employment,” and “rent.” Sentiment analysis was conducted using VADER, focusing on changes across administrations, particularly for topics like housing shortages and delays in benefits access. Keywords with fewer than 10 occurrences were excluded as less relevant. Surprisingly low counts for “migration” and “Blasio” reflected possible shifts in public focus."
      ],
      "metadata": {
        "id": "-yjNppkqmjcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.5 Open Data 311 Service Requests Analysis\n",
        "\n",
        "I applied a similar process to 311 service request data, cleaning the dataset and extracting top keywords to compare trends between administrations. Sentiment analysis was not performed, as complaint descriptors were already categorized and concise."
      ],
      "metadata": {
        "id": "igzF86gTmvPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2.6 Visualizations\n",
        "I created visualizations to track word distributions and trends over time, including a heatmap contrasting sentiment between the de Blasio (higher sentiment) and Adams (lower sentiment) administrations. Word clouds segmented by administration highlighted key terms, providing context for shifts in public concerns and service request patterns."
      ],
      "metadata": {
        "id": "w7p120zJnOUX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0tQmVvrSSEi"
      },
      "outputs": [],
      "source": [
        "!pip install asyncpraw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gExL1gTDSttz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import asyncpraw\n",
        "import asyncio\n",
        "import time\n",
        "\n",
        "reddit = asyncpraw.Reddit(\n",
        "    client_id=\"xb9MZu2WxExu2aLo94v8rA\",\n",
        "    client_secret=\"4s-xlQ0vw1qnK7Pa06o-tc8as4h0Yw\",\n",
        "    user_agent=\"reddit_text_extractor (by u/Good-Bread-994)\"\n",
        ")\n",
        "\n",
        "# Subreddits to fetch data from\n",
        "subreddits = [\"nyc\", \"AskNYC\", \"newyorkcity\", \"NYChousing\", \"311\"]\n",
        "\n",
        "# Function to fetch posts and their top parent comment\n",
        "async def fetch_posts(subreddits):\n",
        "    posts = []\n",
        "\n",
        "    # Loop through each subreddit\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = await reddit.subreddit(subreddit_name)  # Await the subreddit coroutine\n",
        "        async for submission in subreddit.top(limit=500):  # Adjust the limit if needed\n",
        "            # Ensure submission is fully loaded\n",
        "            await submission.load()\n",
        "\n",
        "            # Fetch the top parent comment\n",
        "            submission.comments.replace_more(limit=0)  # Replace \"more comments\" with the actual comments\n",
        "            top_comment = submission.comments[0].body if submission.comments else \"No comments\"\n",
        "\n",
        "            posts.append({\n",
        "                'subreddit': subreddit_name,\n",
        "                'title': submission.title,\n",
        "                'selftext': submission.selftext,\n",
        "                'created_utc': submission.created_utc,\n",
        "                'top_parent_comment': top_comment,  # Add the top parent comment\n",
        "            })\n",
        "\n",
        "    return posts\n",
        "\n",
        "# Function to run the async task and save to a CSV\n",
        "async def main():\n",
        "    posts = await fetch_posts(subreddits)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(posts)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv('reddit_threads_with_parent_comments.csv', index=False)\n",
        "    print(\"Data saved to 'reddit_threads_with_parent_comments.csv'\")\n",
        "\n",
        "    # Close the reddit session to avoid unclosed session warning\n",
        "    await reddit.close()\n",
        "\n",
        "# In Google Colab, use await directly:\n",
        "await main()\n",
        "#The following script is adapted from a Medium tutorial on building a Reddit Scraper with Python and Colab: https://python.plainenglish.io/two-step-wsb-scraper-with-colab-b240af5a6105 and from Melanie Walsh's Reddit Data Collection and Analysis: https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/14-Reddit-Data.html\n",
        "#I prompted ChatGPT to add in code to prevent the Google Colab from timing out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqScPCWCxXmj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPwxdQFNW1Ce"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Python F24/Final Project/reddit_threads_with_parent_comments - Use for Project.csv')\n",
        "\n",
        "df['created_time'] = pd.to_datetime(df['created_utc'], unit='s')\n",
        "\n",
        "reddit_df = df[(df['created_time'].dt.year >= 2020) & (df['created_time'].dt.year <= 2023)]\n",
        "#I adapted this script for the above and below codes from Rebecca Krisel's Intro to Pandas Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5_qf5XqXFpJ"
      },
      "outputs": [],
      "source": [
        "reddit_df.sample(100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqJNkrCvkuPP"
      },
      "outputs": [],
      "source": [
        "reddit_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4Klu95xnOye"
      },
      "outputs": [],
      "source": [
        "reddit_df['created_utc'] = pd.to_datetime(reddit_df['created_utc'],  unit='s')\n",
        "reddit_df['created_utc_str'] = reddit_df['created_utc'].dt.strftime('%Y-%m-%d')\n",
        "#I adapted this code from a subreddit about converting UTC to datetime (Pandas): https://www.reddit.com/r/learnpython/comments/q322no/help_with_converting_utc_to_datetime_pandas/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tWL5OmhnbrT"
      },
      "outputs": [],
      "source": [
        "reddit_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIx12CqnpMWt"
      },
      "outputs": [],
      "source": [
        "reddit_df[reddit_df.duplicated(keep=False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEEOVGhJpcDc"
      },
      "outputs": [],
      "source": [
        "reddit_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O52BAv_pfRN"
      },
      "outputs": [],
      "source": [
        "reddit_df=reddit_df.rename(columns={'created_utc_str':'date', 'selftext':'textpost'})\n",
        "reddit_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE0KjncAqgN5"
      },
      "outputs": [],
      "source": [
        "reddit_drop_date_df = reddit_df.drop(columns=['created_utc', 'created_time'])\n",
        "reddit_drop_date_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYv8ufbduHXB"
      },
      "outputs": [],
      "source": [
        "reddit_drop_date_df.sort_values(by='date', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiaTS2BnxDr6"
      },
      "outputs": [],
      "source": [
        "print(reddit_drop_date_df['date'].min(), reddit_drop_date_df['date'].max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_SSrX2VyEbL"
      },
      "outputs": [],
      "source": [
        "reddit_drop_date_df.groupby('subreddit').agg({'title': 'count', 'textpost': 'count', 'top_parent_comment': 'count', 'date': 'first'}).sort_values(by='title', ascending=False)"
      ]
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert 'date' to datetime format if it's not already\n",
        "reddit_drop_date_df['date'] = pd.to_datetime(reddit_drop_date_df['date'])\n",
        "\n",
        "# Group by week and month, counting the number of posts\n",
        "weekly_trend = reddit_drop_date_df.groupby(reddit_drop_date_df['date'].dt.to_period('W')).size()\n",
        "monthly_trend = reddit_drop_date_df.groupby(reddit_drop_date_df['date'].dt.to_period('M')).size()\n",
        "\n",
        "# Plotting weekly trend\n",
        "monthly_trend.plot(kind='line')\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Monthly Trend of Reddit Posts 2020-2023')  # Title of the plot\n",
        "plt.xlabel('Month')  # Label for the x-axis\n",
        "plt.ylabel('Number of Posts')  # Label for the y-axis\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()  # Adjust the layout for better readability\n",
        "plt.show()\n",
        "#I adapted the scripts to produce the monthly and annual trends from Rebecca Krisel's Intro to Pandas Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PYkjaJQCtfye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9Njif2hHiKU"
      },
      "outputs": [],
      "source": [
        "annual_trend = reddit_drop_date_df.groupby(reddit_drop_date_df['date'].dt.to_period('A')).size()\n",
        "\n",
        "annual_trend.plot(kind='line')\n",
        "\n",
        "plt.title('Annual Trend of Reddit Posts')  # Title of the plot\n",
        "plt.xlabel('Year')  # Label for the x-axis\n",
        "plt.ylabel('Number of Posts')  # Label for the y-axis\n",
        "\n",
        "plt.tight_layout()  # Adjust the layout for better readability\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TU-Y7Bp7GJn"
      },
      "outputs": [],
      "source": [
        "result = reddit_drop_date_df.apply(lambda row: row.astype(str).str.contains('311', case=False).any(), axis=1)\n",
        "filtered_df = reddit_drop_date_df[result]\n",
        "\n",
        "total_count = len(filtered_df)\n",
        "\n",
        "# Print the total count\n",
        "print(\"Total count of rows containing '311':\", total_count)\n",
        "#I used ChatGPT to build this script using the following prompts: find the number of rows containing 311 in the reddit_drop_date_df"
      ]
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab model\n",
        "\n",
        "\n",
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    if pd.isna(text):  # Check if the value is NaN\n",
        "        return ''  # Return an empty string for NaN values\n",
        "    tokens = word_tokenize(str(text))  # Tokenize the text\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalnum()]  # Remove stopwords and punctuation\n",
        "    return ' '.join(filtered_tokens)  # Join tokens back into a string\n",
        "\n",
        "# List of columns to clean\n",
        "columns_to_clean = ['top_parent_comment', 'title', 'textpost']  # Replace with your column names\n",
        "\n",
        "# Process each column\n",
        "for column in columns_to_clean:\n",
        "    if column in reddit_drop_date_df.columns:  # Ensure column exists in the DataFrame\n",
        "        reddit_drop_date_df[f'cleaned_{column}'] = reddit_drop_date_df[column].apply(tokenize_and_remove_stopwords)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(reddit_drop_date_df.head())\n",
        "# I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb\n",
        "##I used ChatGPT to build this script using the following prompts: update the code to hanlde NaN values."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xzk0NboCjdd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VHEIizQ-QLC"
      },
      "outputs": [],
      "source": [
        "reddit_drop_date_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSN346kOMVXo"
      },
      "outputs": [],
      "source": [
        "reddit_short_df = reddit_drop_date_df.drop(columns=['title', 'textpost','top_parent_comment',])\n",
        "reddit_short_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLUITBZ0coZ0"
      },
      "outputs": [],
      "source": [
        "# Create a new column 'combined_text' that concatenates 'cleaned_title' and 'cleaned_textpost'\n",
        "reddit_short_df['combined_text'] = reddit_short_df['cleaned_title'] + ' ' + reddit_short_df['cleaned_textpost']\n",
        "\n",
        "# Display the updated DataFrame with the new 'combined_text' column\n",
        "print(reddit_short_df[['cleaned_title', 'cleaned_textpost', 'combined_text']].head())\n",
        "\n",
        "#I used ChatGPT to build this script using the following prompts: conatenate cleaned_title and cleaned_textpost and create a new column called combined_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_short_df"
      ],
      "metadata": {
        "id": "k0NEF1bGbXKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2U0vx9Jcwpa"
      },
      "outputs": [],
      "source": [
        "reddit_short_df = reddit_short_df.drop(columns=['cleaned_textpost', 'cleaned_title'])\n",
        "\n",
        "print(reddit_short_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux-R8MX6dU6S"
      },
      "outputs": [],
      "source": [
        "reddit_short_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to separate by two time periods."
      ],
      "metadata": {
        "id": "tVeGuB0Ojt21"
      }
    },
    {
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Convert start_date and end_date to datetime64[ns]\n",
        "start_date_deblasio = pd.to_datetime('2020-01-01').to_numpy().astype('datetime64[ns]')\n",
        "end_date_deblasio = pd.to_datetime('2021-12-31').to_numpy().astype('datetime64[ns]')\n",
        "\n",
        "start_date_adams = pd.to_datetime('2022-01-01').to_numpy().astype('datetime64[ns]')\n",
        "end_date_adams = pd.to_datetime('2023-12-31').to_numpy().astype('datetime64[ns]')\n",
        "\n",
        "# Filter the DataFrame\n",
        "deblasio_df = reddit_short_df[(reddit_short_df['date'] >= start_date_deblasio) & (reddit_short_df['date'] <= end_date_deblasio)]\n",
        "adams_df = reddit_short_df[(reddit_short_df['date'] >= start_date_adams) & (reddit_short_df['date'] <= end_date_adams)]\n",
        "\n",
        "\n",
        "# Combine text columns into a single string\n",
        "text_columns = ['cleaned_top_parent_comment', 'combined_text']\n",
        "combined_text_deblasio = deblasio_df[text_columns].fillna('').agg(' '.join, axis=1).str.cat(sep=' ')\n",
        "combined_text_adams = adams_df[text_columns].fillna('').agg(' '.join, axis=1).str.cat(sep=' ')\n",
        "\n",
        "tokens_deblasio = word_tokenize(combined_text_deblasio)\n",
        "tokens_adams = word_tokenize(combined_text_adams)\n",
        "\n",
        "# Extend the stopword list\n",
        "custom_stop_words = {'nan', 'https', 'like', 'nyc', 'get', 'city', 'know', 'got', 'going', 'york', 'really', 'also', 'new', 'nan'}\n",
        "stop_words = set(stopwords.words('english')).union(custom_stop_words)\n",
        "\n",
        "# Clean the tokens\n",
        "tokens_deblasio = [word.lower() for word in tokens_deblasio if word.isalnum() and word.lower() not in stop_words]\n",
        "tokens_adams = [word.lower() for word in tokens_adams if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "# Generate frequency distribution\n",
        "word_counts_deblasio = Counter(tokens_deblasio)\n",
        "word_counts_adams = Counter(tokens_adams)\n",
        "\n",
        "# Get the top 20 keywords\n",
        "top_keywords_deblasio = word_counts_deblasio.most_common(20)\n",
        "top_keywords_adams = word_counts_adams.most_common(20)\n",
        "\n",
        "# Display the results\n",
        "print(\"Top 20 Keywords De blasio:\")\n",
        "for word1, count1 in top_keywords_deblasio:\n",
        "    print(f\"{word1}: {count1}\")\n",
        "\n",
        "print(\"Top 20 Keywords Adams:\")\n",
        "for word, count in top_keywords_adams:\n",
        "    print(f\"{word}: {count}\")\n",
        "#I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb\n",
        "#I used ChatGPT to build this script using the following prompts: update the code with two timeframes from Jan 1, 2020  - December 31, 2021 and Jan 1,2022 - December 31, 2023. Find the most common words, tokenize the data, remove stop words, and generate a custom stop word list"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5s5ncebxxuty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "deblasio_df_key = pd.DataFrame(top_keywords_deblasio, columns=['Keyword', 'De Blasio']).set_index('Keyword')\n",
        "adams_df_key = pd.DataFrame(top_keywords_adams, columns=['Keyword', 'Adams']).set_index('Keyword')\n",
        "\n",
        "# Merge the two DataFrames to create comparison_df\n",
        "comparison_df = pd.merge(deblasio_df_key, adams_df_key, on='Keyword', how='outer').fillna(0)\n",
        "\n",
        "# Calculate the percentage difference\n",
        "comparison_df['Percentage Difference'] = (\n",
        "    (comparison_df['Adams'] - comparison_df['De Blasio']) / comparison_df['De Blasio']\n",
        ") * 100\n",
        "\n",
        "# Replace infinite or NaN values (from division by zero) with a placeholder like 0 or \"N/A\"\n",
        "comparison_df['Percentage Difference'] = comparison_df['Percentage Difference'].replace([float('inf'), -float('inf'), float('nan')], 0)\n",
        "\n",
        "# Sort by the percentage difference if desired\n",
        "comparison_df = comparison_df.sort_values(by='Percentage Difference', ascending=False)\n",
        "\n",
        "# Display the comparison table\n",
        "print(comparison_df)\n",
        "#I shared the frequency distribution with ChatGPT and asked it to calculate the percentage difference between the two time periods with a table."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bro78WYpqXyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deblasio_df"
      ],
      "metadata": {
        "id": "iESIAYU73a_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adams_df"
      ],
      "metadata": {
        "id": "y_pdReQw3T6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_text_adams"
      ],
      "metadata": {
        "id": "bnbVAsJv0esH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_text_deblasio"
      ],
      "metadata": {
        "id": "8ULNVBxW0hOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY9DBtNwVZpf"
      },
      "outputs": [],
      "source": [
        "key_words = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "print(key_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V39B51IXcNz"
      },
      "outputs": [],
      "source": [
        "keyword_frequencies_deblasio = {}\n",
        "keyword_frequencies_adams = {}\n",
        "for keyword in key_words:\n",
        "\n",
        "    keyword_frequencies_deblasio[keyword] = tokens_deblasio.count(keyword.lower())\n",
        "    keyword_frequencies_adams[keyword] = tokens_adams.count(keyword.lower())\n",
        "print(\"Keyword Frequencies de Blasio:\")\n",
        "for keyword, frequency in keyword_frequencies_deblasio.items():\n",
        "    print(f\"{keyword}: {frequency}\")\n",
        "print(\"Keyword Frequencies Adams:\")\n",
        "for keyword, frequency in keyword_frequencies_adams.items():\n",
        "    print(f\"{keyword}: {frequency}\")\n",
        "#I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjt4SpbmjA2m"
      },
      "outputs": [],
      "source": [
        "# Organize the keyword frequencies from smallest to largest\n",
        "sorted_frequencies_adams = sorted(keyword_frequencies_adams.items(), key=lambda x: x[1])\n",
        "sorted_frequencies_deblasio = sorted(keyword_frequencies_deblasio.items(), key=lambda x: x[1])\n",
        "\n",
        "print(\"Sorted Keyword Frequencies Adams (Smallest to Largest):\")\n",
        "for keyword, frequency in sorted_frequencies_adams:\n",
        "    print(f\"{keyword}: {frequency}\")\n",
        "print(\"Sorted Keyword Frequencies de Blasio (Smallest to Largest):\")\n",
        "for keyword, frequency in sorted_frequencies_deblasio:\n",
        "    print(f\"{keyword}: {frequency}\")\n",
        "#I prompted ChatGPT to organize the keyword frequencies from smallest to largest."
      ]
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define keywords and initialize frequency dictionaries\n",
        "key_words = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "# Initialize the keyword frequency dictionaries with 0 counts for each keyword\n",
        "keyword_frequencies_deblasio = {keyword: 0 for keyword in key_words}\n",
        "keyword_frequencies_adams = {keyword: 0 for keyword in key_words}\n",
        "\n",
        "for keyword in key_words:\n",
        "\n",
        "    keyword_frequencies_deblasio[keyword] = tokens_deblasio.count(keyword.lower())\n",
        "    keyword_frequencies_adams[keyword] = tokens_adams.count(keyword.lower())\n",
        "\n",
        "\n",
        "keywords = key_words\n",
        "de_blasio_frequencies = [keyword_frequencies_deblasio[keyword] for keyword in key_words]\n",
        "adams_frequencies = [keyword_frequencies_adams[keyword] for keyword in key_words]\n",
        "\n",
        "# Sort the frequencies\n",
        "sorted_frequencies_adams = dict(sorted(keyword_frequencies_adams.items(), key=lambda item: item[1], reverse=True))\n",
        "sorted_frequencies_deblasio = dict(sorted(keyword_frequencies_deblasio.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Plot the first chart for De Blasio\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(sorted_frequencies_deblasio.keys(), sorted_frequencies_deblasio.values(), color='skyblue')\n",
        "plt.xlabel('Keywords', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Frequency of Keywords in Cleaned Text - De Blasio', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot the second chart for Adams\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(sorted_frequencies_adams.keys(), sorted_frequencies_adams.values(), color='lightcoral')\n",
        "plt.xlabel('Keywords', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Frequency of Keywords in Cleaned Text - Adams', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb and I prompted ChatGPT to incorporate my keyword list and create a bar chart with the keyword frequencies"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jOmgeZEKplto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ShR3r11f1B5"
      },
      "source": [
        "##Sentiment Analysis\n",
        "I used VADER, designed for short social media texts to capture subtle positive and negative tones in comments, to conduct the sentiment analysis on my keyword list. I downloaded the NLTK library for keyword extraction and co-occurrence analysis to reveal different trends in complaints across the shift from de Blasio to Adams. I segmented the posts by administration to capture if negativity increases around topics like housing shortages, covid, and benefits access delays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('all')\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "#I adapted this script from Melanie Walsh's Sentiment Analysis: https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/04-Sentiment-Analysis.html"
      ],
      "metadata": {
        "id": "RQevBTqDv6Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(text):\n",
        "    # Get the sentiment scores for the text\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "\n",
        "    # Return the compound sentiment score (this score reflects the overall sentiment)\n",
        "    return scores['compound']\n",
        "\n",
        "# Ensure you're working with a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "adams_df = adams_df.copy()\n",
        "deblasio_df = deblasio_df.copy()\n",
        "\n",
        "# Apply the get_sentiment function using .loc to avoid SettingWithCopyWarning\n",
        "adams_df.loc[:, 'combined_text_sentiment'] = adams_df['combined_text'].apply(get_sentiment)\n",
        "adams_df.loc[:, 'parent_comment_sentiment'] = adams_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "deblasio_df.loc[:, 'combined_text_sentiment'] = deblasio_df['combined_text'].apply(get_sentiment)\n",
        "deblasio_df.loc[:, 'parent_comment_sentiment'] = deblasio_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "\n",
        "# Adjust display settings for horizontal output\n",
        "pd.set_option('display.max_colwidth', 100)  # Truncate long columns for better readability\n",
        "pd.set_option('display.expand_frame_repr', False)  # Prevent line wrapping for wide DataFrames\n",
        "\n",
        "# Adams Period Sentiment Analysis - Set index to comment ID for better readability\n",
        "adams_display = adams_df[['combined_text', 'combined_text_sentiment', 'cleaned_top_parent_comment', 'parent_comment_sentiment']]\n",
        "adams_display = adams_display.reset_index(drop=True)\n",
        "\n",
        "# De Blasio Period Sentiment Analysis - Set index to comment ID for better readability\n",
        "deblasio_display = deblasio_df[['combined_text', 'combined_text_sentiment', 'cleaned_top_parent_comment', 'parent_comment_sentiment']]\n",
        "deblasio_display = deblasio_display.reset_index(drop=True)\n",
        "\n",
        "# Print Results\n",
        "print(\"Adams Period Sentiment Analysis:\")\n",
        "print(adams_display)\n",
        "\n",
        "print(\"\\nDe Blasio Period Sentiment Analysis:\")\n",
        "print(deblasio_display)\n",
        "# Using Melanie Walsh's Sentiment Analysis guide, I prompted ChatGPT to:\n",
        "# - write a code to apply the sentiment analysis to my two dataframes\n",
        "# - apply a sentiment analysis for the columns combined_text and cleaned_top_parent_comment for both Adams and de Blasio\n",
        "# - display the results side by side"
      ],
      "metadata": {
        "id": "IF3xkCv-wX5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPwRHE_02ars"
      },
      "source": [
        "To analyze the average sentiment, I selected the key words from my initial hypthosis with a frequency of greater than 10. Anything under 10 was dropped, as the terms were less relevant than originally expected. The words I removed from my key word search were: immigration, migrant, Blasio, and employment."
      ]
    },
    {
      "source": [
        "deblasio_df = reddit_short_df[(reddit_short_df['date'] >= start_date_deblasio) & (reddit_short_df['date'] <= end_date_deblasio)].copy()\n",
        "adams_df = reddit_short_df[(reddit_short_df['date'] >= start_date_adams) & (reddit_short_df['date'] <= end_date_adams)].copy()\n",
        "\n",
        "# Initialize the keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delay', 'budget', 'covid', 'adams', 'wage', 'rent']\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Create the get_sentiment function\n",
        "def get_sentiment(text):\n",
        "    # Get the sentiment scores for the text\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "\n",
        "    # Return the compound sentiment score (this score reflects the overall sentiment)\n",
        "    return scores['compound']\n",
        "\n",
        "# Apply the get_sentiment function to both DataFrames, creating 'post_sentiment'\n",
        "adams_df['post_sentiment'] = adams_df['combined_text'].apply(get_sentiment) #Creating post_sentiment column\n",
        "deblasio_df['post_sentiment'] = deblasio_df['combined_text'].apply(get_sentiment) #Creating post_sentiment column\n",
        "\n",
        "# Function to calculate average sentiment for each keyword\n",
        "def calculate_average_sentiment(df, keywords, sentiment_column):\n",
        "    average_sentiments = {}\n",
        "\n",
        "    for keyword in keywords:\n",
        "        # Filter rows where the keyword is present in 'combined_text'\n",
        "        # using str.contains for case-insensitive search\n",
        "        keyword_rows = df[df['combined_text'].str.contains(keyword, case=False, na=False)]\n",
        "\n",
        "        # Calculate the average sentiment score for these rows\n",
        "        avg_sentiment = keyword_rows[sentiment_column].mean()\n",
        "        average_sentiments[keyword] = avg_sentiment\n",
        "\n",
        "    return average_sentiments\n",
        "\n",
        "# Calculate average sentiment for Adams period\n",
        "adams_average_sentiments = calculate_average_sentiment(adams_df, keywords, 'post_sentiment')\n",
        "\n",
        "# Calculate average sentiment for De Blasio period\n",
        "deblasio_average_sentiments = calculate_average_sentiment(deblasio_df, keywords, 'post_sentiment')\n",
        "\n",
        "# Print the average sentiment for each keyword for both periods\n",
        "print(\"Average Post Sentiment for Adams Period:\")\n",
        "for keyword, avg_sentiment in adams_average_sentiments.items():\n",
        "    print(f\"{keyword}: {avg_sentiment}\")\n",
        "\n",
        "print(\"\\nAverage Post Sentiment for De Blasio Period:\")\n",
        "for keyword, avg_sentiment in deblasio_average_sentiments.items():\n",
        "    print(f\"{keyword}: {avg_sentiment}\")\n",
        "\n",
        "#Using Melanie Walsh's Sentiment Analysis guide, I prompted ChatGPT to write a code to apply the sentiment analysis to my two dataframes and the post_sentiment column"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JOfmYdfBsdan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt  # Ensure you have plt imported for the plot\n",
        "\n",
        "# Define the keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delay', 'budget', 'covid', 'adams', 'wage', 'rent']\n",
        "\n",
        "# Create a DataFrame\n",
        "# Convert dictionaries to lists for 'Adams Sentiment' and 'De Blasio Sentiment' columns\n",
        "data = {\n",
        "    'Keyword': keywords,\n",
        "    'Adams Sentiment': [adams_average_sentiments.get(keyword, float('nan')) for keyword in keywords],\n",
        "    'De Blasio Sentiment': [deblasio_average_sentiments.get(keyword, float('nan')) for keyword in keywords]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plotting heatmap\n",
        "df.set_index('Keyword', inplace=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.T, annot=True, cmap=\"coolwarm\", center=0, cbar_kws={'label': 'Sentiment'})\n",
        "plt.title('Sentiment Heatmap: Adams vs De Blasio')\n",
        "plt.show()\n",
        "\n",
        "#I adapted the above script from Geeks for Geeks Seaborn Heatmap Guide: https://www.geeksforgeeks.org/seaborn-heatmap-a-comprehensive-guide/\n",
        "#I prompted ChatGPT to modify the code with my keyword list and plot a heatmap using the average sentiment of the two time periods, being sure to remove nan"
      ],
      "metadata": {
        "id": "dSRS7LnL1pFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUie86pZ5HEF"
      },
      "source": [
        "To analyze the sentiment of the keywords with a frequency distribution greater than 10, I took the average sentiment from the post sentiment column. Budget and Wage have the highest sentiment score. This likely alludes to the liminations of sentiment analysis - both of these keywords were viewed positively in the press or in anlysis of the mayors' administrations. Budget, particularly during the Adams administration was highly criticized as he cut the budget of all agencies by 5% and threated to significantly reduce library and senior services. Benefits had the highest sentiment score at 0.99, which could highlight the increase in benefits during the Covid pandemic or just show that Reddit users speak positively about benefits offered in NYC. Wage also had a high sentiment score under de Blasio, surprising given the percentage of people unemployed.\n",
        "\n",
        "Keywords that received the lowest sentiment scores were Adams at 0.004 and shelter at 0.13. The low rating of shelter from 2020 - 2023 aligns with my hypothesis that shelter would be perceived as negative during the timeframe. This is when shelters were at capacity due to the migrant crisis. Adams also has the lowest sentiment score."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deblasio_df = reddit_short_df[(reddit_short_df['date'] >= start_date_deblasio) & (reddit_short_df['date'] <= end_date_deblasio)].copy()\n",
        "adams_df = reddit_short_df[(reddit_short_df['date'] >= start_date_adams) & (reddit_short_df['date'] <= end_date_adams)].copy()\n",
        "\n",
        "# Initialize the keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delay', 'budget', 'covid', 'adams', 'wage', 'rent']\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Create the get_sentiment function\n",
        "def get_sentiment(text):\n",
        "    # Get the sentiment scores for the text\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "\n",
        "    # Return the compound sentiment score (this score reflects the overall sentiment)\n",
        "    return scores['compound']\n",
        "\n",
        "# Apply the get_sentiment function to both DataFrames\n",
        "adams_df['parent_comment_sentiment'] = adams_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "deblasio_df['parent_comment_sentiment'] = deblasio_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "\n",
        "# Function to calculate average sentiment for each keyword for both periods\n",
        "def calculate_average_sentiment(df, keywords, sentiment_column):\n",
        "    average_sentiments = {}\n",
        "\n",
        "    for keyword in keywords:\n",
        "        # Filter rows where the keyword is present in 'combined_text'\n",
        "        # using str.contains for case-insensitive search\n",
        "        keyword_rows = df[df['combined_text'].str.contains(keyword, case=False, na=False)]\n",
        "\n",
        "        # Calculate the average sentiment score for these rows\n",
        "        avg_sentiment = keyword_rows[sentiment_column].mean()\n",
        "        average_sentiments[keyword] = avg_sentiment\n",
        "\n",
        "    return average_sentiments\n",
        "\n",
        "# Calculate average sentiment for Adams period\n",
        "adams_average_sentiments = calculate_average_sentiment(adams_df, keywords, 'parent_comment_sentiment')\n",
        "\n",
        "# Calculate average sentiment for De Blasio period\n",
        "deblasio_average_sentiments = calculate_average_sentiment(deblasio_df, keywords, 'parent_comment_sentiment')\n",
        "\n",
        "# Print the average sentiment for Adams period\n",
        "print(\"Average Parent Comment Sentiment for Adams Period:\")\n",
        "for keyword, sentiment in adams_average_sentiments.items():\n",
        "    print(f\"{keyword}: {sentiment}\")\n",
        "\n",
        "# Print a separator line\n",
        "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "\n",
        "# Print the average sentiment for De Blasio period\n",
        "print(\"Average Parent Comment Sentiment for De Blasio Period:\")\n",
        "for keyword, sentiment in deblasio_average_sentiments.items():\n",
        "    print(f\"{keyword}: {sentiment}\")\n",
        "\n",
        "#Using Melanie Walsh's Sentiment Analysis guide, I prompted ChatGPT to write a code to include the parent_comment sentiment for the Adams time period and de Blasio time period.\n",
        "# I also prompted ChatGPT to print a separator line to distinguish between the two time periods"
      ],
      "metadata": {
        "id": "aJTKtU_htmDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Define the keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delay', 'budget', 'covid', 'adams', 'wage', 'rent']\n",
        "\n",
        "# Create a DataFrame\n",
        "# Convert dictionaries to lists for 'Adams Sentiment' and 'De Blasio Sentiment' columns\n",
        "data = {\n",
        "    'Keyword': keywords,\n",
        "    'Adams Sentiment': [adams_average_sentiments.get(keyword, float('nan')) for keyword in keywords],\n",
        "    'De Blasio Sentiment': [deblasio_average_sentiments.get(keyword, float('nan')) for keyword in keywords]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plotting heatmap\n",
        "df.set_index('Keyword', inplace=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.T, annot=True, cmap=\"coolwarm\", center=0, cbar_kws={'label': 'Sentiment'})\n",
        "plt.title('Sentiment Heatmap: Adams vs De Blasio')\n",
        "plt.show()\n",
        "\n",
        "#I adapted the above script from Geeks for Geeks Seaborn Heatmap Guide: https://www.geeksforgeeks.org/seaborn-heatmap-a-comprehensive-guide/\n",
        "#I prompted ChatGPT to modify the code with my keyword list\n",
        "#I prompted ChatGPT to create a dictionary data with three keys: 'Keyword', 'Adams Sentiment', and 'De Blasio Sentiment'"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fyF537NR8vZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure DataFrame slices are explicit copies to avoid SettingWithCopyWarning\n",
        "deblasio_df = reddit_short_df[(reddit_short_df['date'] >= start_date_deblasio) & (reddit_short_df['date'] <= end_date_deblasio)].copy()\n",
        "adams_df = reddit_short_df[(reddit_short_df['date'] >= start_date_adams) & (reddit_short_df['date'] <= end_date_adams)].copy()\n",
        "\n",
        "# Initialize the keywords list\n",
        "keywords =  ['housing', 'benefits', 'shelter', 'wait', 'delay', 'budget', 'covid', 'adams', 'wage', 'rent']\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Create the get_sentiment function\n",
        "def get_sentiment(text):\n",
        "    # Get the sentiment scores for the text\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "\n",
        "    # Return the compound sentiment score (this score reflects the overall sentiment)\n",
        "    return scores['compound']\n",
        "\n",
        "# Apply the get_sentiment function to both DataFrames, creating 'post_sentiment'\n",
        "adams_df['post_sentiment'] = adams_df['combined_text'].apply(get_sentiment) #Creating post_sentiment column\n",
        "adams_df['parent_comment_sentiment'] = adams_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "deblasio_df['post_sentiment'] = deblasio_df['combined_text'].apply(get_sentiment) #Creating post_sentiment column\n",
        "deblasio_df['parent_comment_sentiment'] = deblasio_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "\n",
        "# Function to calculate average sentiment for each keyword across both parent_comment_sentiment and post_sentiment\n",
        "def calculate_average_sentiment(df, keywords):\n",
        "    average_sentiments = {}\n",
        "\n",
        "    for keyword in keywords:\n",
        "        # Filter rows where the keyword is present in 'combined_text'\n",
        "        keyword_rows = df[df['combined_text'].str.contains(keyword, case=False, na=False)].copy()  # Explicit copy to avoid SettingWithCopyWarning\n",
        "\n",
        "        # Calculate the average sentiment score for these rows across both columns (parent_comment_sentiment and post_sentiment)\n",
        "        keyword_rows['average_sentiment'] = keyword_rows[['parent_comment_sentiment', 'post_sentiment']].mean(axis=1)\n",
        "\n",
        "        # Calculate the average of the combined sentiment for this keyword\n",
        "        avg_sentiment = keyword_rows['average_sentiment'].mean()\n",
        "        average_sentiments[keyword] = avg_sentiment\n",
        "\n",
        "    return average_sentiments\n",
        "\n",
        "# Calculate average sentiment for Adams period\n",
        "adams_average_sentiments = calculate_average_sentiment(adams_df, keywords)\n",
        "\n",
        "# Calculate average sentiment for De Blasio period\n",
        "deblasio_average_sentiments = calculate_average_sentiment(deblasio_df, keywords)\n",
        "\n",
        "# Print the average sentiment for each keyword for both periods\n",
        "print(\"Average Sentiment for Adams Period (combined):\")\n",
        "for keyword, avg_sentiment in adams_average_sentiments.items():\n",
        "    print(f\"{keyword}: {avg_sentiment}\")\n",
        "\n",
        "print(\"\\nAverage Sentiment for De Blasio Period (combined):\")\n",
        "for keyword, avg_sentiment in deblasio_average_sentiments.items():\n",
        "    print(f\"{keyword}: {avg_sentiment}\")\n",
        "\n",
        "#Using Melanie Walsh's Sentiment Analysis guide, I prompted ChatGPT to write a code to average the sentiment for the Adams time period and de Blasio time period.\n",
        "#I also prompted ChatGPT to slice the dataframe by date due to settingwithwarning errors.\n",
        "#I prompted ChatGPT to find the average sentimennt for keywords and produce an average sentiment score"
      ],
      "metadata": {
        "id": "552bM_vi0oc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the keywords list\n",
        "keywords =  ['housing', 'benefits', 'shelter', 'wait', 'delay', 'budget', 'covid', 'adams', 'wage', 'rent']\n",
        "\n",
        "# Create a DataFrame\n",
        "# Convert dictionaries to lists for 'Adams Sentiment' and 'De Blasio Sentiment' columns\n",
        "data = {\n",
        "    'Keyword': keywords,\n",
        "    'Adams Sentiment': [adams_average_sentiments.get(keyword, float('nan')) for keyword in keywords],\n",
        "    'De Blasio Sentiment': [deblasio_average_sentiments.get(keyword, float('nan')) for keyword in keywords]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plotting heatmap\n",
        "df.set_index('Keyword', inplace=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.T, annot=True, cmap=\"coolwarm\", center=0, cbar_kws={'label': 'Sentiment'})\n",
        "plt.title('Sentiment Heatmap: Adams vs De Blasio')\n",
        "plt.show()\n",
        "\n",
        "#I adapted the above script from Geeks for Geeks Seaborn Heatmap Guide: https://www.geeksforgeeks.org/seaborn-heatmap-a-comprehensive-guide/ and I prompted ChatGPT to modify the code with my keyword list"
      ],
      "metadata": {
        "id": "w3S43l_Z091E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQerwZkjERo0"
      },
      "source": [
        "Adams has a slightly higher sentiment in parent comments but it is still the lowest of the key words. Benefits decreased significantly, by 0.4 to be 0.51. This could be because posts are generally more question oriented or neutral where as parent comments express a response to the post."
      ]
    },
    {
      "source": [
        "# Keywords list\n",
        "keywords =  ['housing', 'benefits', 'shelter', 'wait', 'delay', 'budget', 'covid', 'adams', 'wage', 'rent']\n",
        "\n",
        "# Function to calculate keyword frequencies for a given DataFrame\n",
        "def calculate_keyword_frequencies(df, keywords):\n",
        "    # Initialize an empty dictionary to store frequencies\n",
        "    keyword_frequencies = {}\n",
        "\n",
        "    # Loop through each keyword\n",
        "    for keyword in keywords:\n",
        "        # Count occurrences of the keyword in 'combined_text' column, ignoring case\n",
        "        keyword_frequencies[keyword] = df['combined_text'].str.contains(keyword, case=False).sum()\n",
        "\n",
        "    return keyword_frequencies\n",
        "\n",
        "# Calculate keyword frequencies for Adams period\n",
        "adams_keyword_frequencies = calculate_keyword_frequencies(adams_df, keywords)\n",
        "\n",
        "# Calculate keyword frequencies for De Blasio period\n",
        "deblasio_keyword_frequencies = calculate_keyword_frequencies(deblasio_df, keywords)\n",
        "\n",
        "# Create and display the word cloud for Adams period\n",
        "wordcloud_adams = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(adams_keyword_frequencies)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_adams, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off the axis\n",
        "plt.title(\"Adams Period - Keyword Frequency Word Cloud\")\n",
        "plt.show()\n",
        "\n",
        "# Create and display the word cloud for De Blasio period\n",
        "wordcloud_deblasio = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(deblasio_keyword_frequencies)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_deblasio, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off the axis\n",
        "plt.title(\"De Blasio Period - Keyword Frequency Word Cloud\")\n",
        "plt.show()\n",
        "\n",
        "#I adapted the above and below code from Geeks for Geeks Generating Word Cloud in Python Guide: https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
        "# I asked ChatGPT to incorporate my key words and their respective frequency distribution"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vE5L76hX9GeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhR4WryA0DLh"
      },
      "source": [
        "#NYC Open Data 311 Service Request Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Python F24/Final Project/311_Service_Requests (2).csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "#I adapted this code from Rebecca Krisel's Data Manipulation in Pandas and Python Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb"
      ],
      "metadata": {
        "id": "z1qWYttb5Qkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "jLdf2ke4zlLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['Location Type'])"
      ],
      "metadata": {
        "id": "RC2D74g6fINI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaSd4OagQEva"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "_ZF4DgEOGFhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Unique Key'] = df['Unique Key'].astype(str)\n",
        "\n",
        "print(df.dtypes)\n"
      ],
      "metadata": {
        "id": "3qZOhWAkWPYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Created Date'].min(), df['Created Date'].max())\n"
      ],
      "metadata": {
        "id": "Yma5zgE4qPC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "df['Created Date'] = pd.to_datetime(df['Created Date'],\n",
        "                                    format='%m/%d/%Y %I:%M:%S %p')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1r2BmK4yeXVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated(keep=False)]"
      ],
      "metadata": {
        "id": "FfWesFZvGNPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_agencies = df['Agency'].unique()\n",
        "\n",
        "print(unique_agencies)\n"
      ],
      "metadata": {
        "id": "XjydOhn7Gv2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I made the decision to include NYPD because calls about unhoused populations often are directed to the NYPD. The other agencies are directly related to benefits, wages, and social services."
      ],
      "metadata": {
        "id": "AtOOjZN3idWM"
      }
    },
    {
      "source": [
        "agencies_to_include = ['DOHMH', 'NYPD', 'HPD', 'DCWP', 'DHS']\n",
        "\n",
        "agencies_df = df[df['Agency'].isin(agencies_to_include)]\n",
        "\n",
        "print(agencies_df)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "s0_CVhLkW9Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am not going to tokenize because the data is already tokenized. Below I decided to remove any descriptor with more than one description because it just provides numbers and not words, not good for analysis."
      ],
      "metadata": {
        "id": "XLiSCM5ge4BA"
      }
    },
    {
      "source": [
        "df_filtered = agencies_df[agencies_df['Descriptor'].apply(lambda x: not bool(pd.notnull(x) and isinstance(x, str) and any(char.isdigit() for char in x)))]\n",
        "\n",
        "print(df_filtered)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lrklxzsvgPp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered"
      ],
      "metadata": {
        "id": "kW0fTdt4pRB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to divide my data into two date ranges per administration and summarize the number of complaints."
      ],
      "metadata": {
        "id": "dNoDljwAhci3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date_range1_start = '2020-01-01'\n",
        "date_range1_end = '2021-12-31'\n",
        "date_range2_start = '2022-01-01'\n",
        "date_range2_end = '2023-12-31'\n",
        "\n",
        "# Filter the DataFrame for each date range\n",
        "df_range1 = df_filtered[(df_filtered['Created Date'] >= date_range1_start) & (df_filtered['Created Date'] <= date_range1_end)]\n",
        "df_range2 = df_filtered[(df_filtered['Created Date'] >= date_range2_start) & (df_filtered['Created Date'] <= date_range2_end)]\n",
        "\n",
        "# Group by 'Agency' and count the complaints\n",
        "complaints_range1 = df_range1['Agency'].value_counts().reset_index()\n",
        "complaints_range1.columns = ['Agency', 'Complaints (2020-2021)']\n",
        "\n",
        "complaints_range2 = df_range2['Agency'].value_counts().reset_index()\n",
        "complaints_range2.columns = ['Agency', 'Complaints (2022-2023)']\n",
        "\n",
        "# Merge the results into a single DataFrame\n",
        "complaints_summary = pd.merge(\n",
        "    complaints_range1,\n",
        "    complaints_range2,\n",
        "    on='Agency',\n",
        "    how='outer'\n",
        ").fillna(0)  # Fill NaN values with 0\n",
        "\n",
        "# Convert counts to integers\n",
        "complaints_summary[['Complaints (2020-2021)', 'Complaints (2022-2023)']] = complaints_summary[\n",
        "    ['Complaints (2020-2021)', 'Complaints (2022-2023)']\n",
        "].astype(int)\n",
        "\n",
        "# Display the summary\n",
        "print(complaints_summary)\n",
        "\n",
        "#I prompted ChatGPT to group the complaints by agency across the two timeframes and print as a table."
      ],
      "metadata": {
        "id": "JqczoFFcpqwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set positions and width for the bars\n",
        "x = np.arange(len(complaints_summary['Agency']))  # label locations\n",
        "width = 0.35  # width of bars\n",
        "\n",
        "# Create the figure and axes\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Bar plots for each date range\n",
        "bars1 = ax.bar(x - width/2, complaints_summary['Complaints (2020-2021)'], width, label='2020-2021')\n",
        "bars2 = ax.bar(x + width/2, complaints_summary['Complaints (2022-2023)'], width, label='2022-2023')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Agency', fontsize=12)\n",
        "ax.set_ylabel('Number of Complaints', fontsize=12)\n",
        "ax.set_title('Number of Complaints by Agency (2020-2023)', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(complaints_summary['Agency'], rotation=45, ha='right')\n",
        "ax.legend()\n",
        "\n",
        "# Add value annotations on top of bars\n",
        "for bar in bars1 + bars2:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height}',\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3),  # Offset text above the bar\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#I adapted this code from Rebecca Krisel's Data Manipulation in Pandas and Python Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb\n",
        "# prompted ChatGPT to modify the code with the two time frames and add values on the top of the bars"
      ],
      "metadata": {
        "id": "ffXeIvAqjPuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "\n",
        "# Ensure stopwords are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab model\n",
        "\n",
        "# Define stop words and punctuation\n",
        "custom_stop_words = ['nan']\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Function to preprocess and get word frequencies\n",
        "def get_top_words(df, column_name, top_n=20):\n",
        "    # Combine all text in the specified column\n",
        "    all_text = ' '.join(df[column_name].dropna().astype(str))\n",
        "\n",
        "    # Tokenize, remove punctuation and stop words, and convert to lowercase\n",
        "    tokens = word_tokenize(all_text)\n",
        "    cleaned_tokens = [\n",
        "        word.lower() for word in tokens\n",
        "        if word.lower() not in stop_words and word not in punctuation and word.isalpha()\n",
        "    ]\n",
        "\n",
        "    # Get word frequencies\n",
        "    freq_dist = FreqDist(cleaned_tokens)\n",
        "    return freq_dist.most_common(top_n)\n",
        "\n",
        "# Get top 20 words for each period\n",
        "df_filtered.loc[:, 'Created Date'] = pd.to_datetime(df_filtered['Created Date'])\n",
        "\n",
        "df_range1 = df_filtered.loc[(df_filtered['Created Date'] >= date_range1_start) & (df_filtered['Created Date'] <= date_range1_end)]\n",
        "df_range2 = df_filtered.loc[(df_filtered['Created Date'] >= date_range2_start) & (df_filtered['Created Date'] <= date_range2_end)]\n",
        "\n",
        "top_words_range1 = get_top_words(df_range1, 'Descriptor', top_n=20)\n",
        "top_words_range2 = get_top_words(df_range2, 'Descriptor', top_n=20)\n",
        "\n",
        "# Print results\n",
        "print(\"Top 20 Words for de Blasio Administration in 311 Data:\")\n",
        "for word, count in top_words_range1:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nTop 20 Words for Adams Administration in 311 Data:\")\n",
        "for word, count in top_words_range2:\n",
        "    print(f\"{word}: {count}\")\n",
        "#I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb\n",
        "#I prompted ChatGPT to update the code with two timeframes, custom stop words, and top 20 words, and to print the results with the top 20 words for each time period."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KBx2b1b0keYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words_range1 = get_top_words(df_range1, 'Descriptor', top_n=20)\n",
        "top_words_range2 = get_top_words(df_range2, 'Descriptor', top_n=20)\n",
        "\n",
        "# Create DataFrames for top words\n",
        "range1_df = pd.DataFrame(top_words_range1, columns=['Word', 'Count'])  # Assign to range1_df\n",
        "range2_df = pd.DataFrame(top_words_range2, columns=['Word', 'Count'])\n",
        "\n",
        "\n",
        "range1_df = range1_df.sort_values(by='Count', ascending=False)\n",
        "range2_df = range2_df.sort_values(by='Count', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range1_df['Word'], range1_df['Count'], color='skyblue')\n",
        "plt.xlabel('Keywords', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Top Keywords in 311 Data During de Blasio Administration', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range2_df['Word'], range2_df['Count'], color='lightcoral')\n",
        "plt.xlabel('Keywords', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Top Keywords in 311 Data During Adams Administration', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#I adapted this script from Rebecca Krisel's Pandas Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb"
      ],
      "metadata": {
        "id": "-wnXiXB5Mk7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_words = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "def count_keywords(df, key_words, text_column):\n",
        "    keyword_counts = {word: 0 for word in key_words}  # Initialize dictionary with 0 counts for each keyword\n",
        "\n",
        "    # Iterate over each row in the DataFrame\n",
        "    for text in df[text_column].dropna():  # Use dropna() to skip NaN values\n",
        "        # Convert text to lowercase for case-insensitive matching\n",
        "        text = text.lower()\n",
        "\n",
        "        # Check each keyword and count its occurrences\n",
        "        for word in key_words:\n",
        "            keyword_counts[word] += text.count(word)  # Count occurrences of the word in the text\n",
        "\n",
        "    return keyword_counts\n",
        "\n",
        "keyword_counts_deblasio = count_keywords(df_range1, key_words, 'Descriptor')\n",
        "keyword_counts_adams = count_keywords(df_range2, key_words, 'Descriptor')\n",
        "\n",
        "print(\"Keyword Counts for de Blasio Administration:\")\n",
        "for word, count in keyword_counts_deblasio.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nKeyword Counts for Adams Administration:\")\n",
        "for word, count in keyword_counts_adams.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "#I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb\n",
        "#I prompted ChatGPT to update the code, counting the occurence of the keywords in the dataframe.\n",
        "#I then asked ChatGPT to analyze keyword frequences for two datasets corresponding to the two time periods and to return the results with the list of frequencies"
      ],
      "metadata": {
        "id": "oS4v8r23pD5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_deblasio_counts = sorted(keyword_counts_deblasio.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_adams_counts = sorted(keyword_counts_adams.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "deblasio_keywords, deblasio_counts = zip(*sorted_deblasio_counts)\n",
        "adams_keywords, adams_counts = zip(*sorted_adams_counts)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(deblasio_keywords, deblasio_counts, color='skyblue')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Keyword Counts for de Blasio Administration\", fontsize=14)\n",
        "plt.xlabel(\"Keywords\", fontsize=12)\n",
        "plt.ylabel(\"Counts\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(adams_keywords, adams_counts, color='lightcoral')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Keyword Counts for Adams Administration\", fontsize=14)\n",
        "plt.xlabel(\"Keywords\", fontsize=12)\n",
        "plt.ylabel(\"Counts\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#I adapted this script from Rebecca Krisel's Pandas Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb"
      ],
      "metadata": {
        "id": "l-Qr_tvsM9ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df_filtered.copy()\n",
        "\n",
        "# Convert 'Created Date' to datetime format if it's not already\n",
        "df_filtered['Created Date'] = pd.to_datetime(df_filtered['Created Date'])\n",
        "\n",
        "# Group by month, counting the number of complaints\n",
        "monthly_trend = df_filtered.groupby(df_filtered['Created Date'].dt.to_period('M')).size()\n",
        "\n",
        "# Set up the figure size for a larger plot\n",
        "plt.figure(figsize=(12, 6))  # Increase the width and height for a larger graph\n",
        "\n",
        "# Plotting the monthly trend\n",
        "monthly_trend.plot(kind='line', color='b', linewidth=2)  # Use smooth lines with consistent color (blue)\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Monthly Trend of Open Data 311 Complaints', fontsize=16)  # Title with larger font size\n",
        "plt.xlabel('Month', fontsize=14)  # Label for the x-axis with larger font size\n",
        "plt.ylabel('Number of Complaints', fontsize=14)  # Label for the y-axis with larger font size\n",
        "\n",
        "# Adjust layout for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate complaints per year before plotting\n",
        "complaints_per_year = df_filtered.groupby(df_filtered['Created Date'].dt.year).size().reset_index(name='Complaints')\n",
        "complaints_per_year.columns = ['Year', 'Complaints']  # Rename columns for clarity\n",
        "\n",
        "# Plotting complaints per year\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(complaints_per_year['Year'], complaints_per_year['Complaints'], color='green')\n",
        "plt.title('311 Service Complaints Per Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Complaints')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()  # Adjust layout for readability\n",
        "plt.show()\n",
        "\n",
        "#I adapted this script from Rebecca Krisel's Pandas Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb\n",
        "# I prompted ChatGPT to preprocess and make a copy of the data to avoid errors and ensure data values are treated as timestamps\n",
        "#I prompted ChatGPT to calculate monthly trends from the dataframe and plot the trend with a line chart\n",
        "#I prompted ChatGPT to plot the complaints per year with a bar chart and adjust for readability."
      ],
      "metadata": {
        "id": "d7YXnIgbztCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Results"
      ],
      "metadata": {
        "id": "Egg-d1SZ0msf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Reddit Results\n",
        "The top 20 most common Reddit keywords (e.g., \"feel,\" \"one,\" \"day,\" \"work\") lacked direct policy relevance, limiting insights. Sorting by a predefined keyword list provided clearer comparisons. During de Blasio's tenure, words like \"immigration,\" \"migrant,\" \"Adams,\" and \"budget\" had fewer than 10 mentions. Similarly, Adams' tenure showed low mentions for \"immigration,\" \"benefits,\" and \"shelter.\" Notably, \"shelter\" was mentioned 11 more times during de Blasio’s period, potentially linked to pandemic-related policies. \"Covid\" appeared 35% more during de Blasio’s term, aligning with its pandemic peak. Surprisingly, \"benefits\" were mentioned 53 times under de Blasio but only 3 under Adams, despite criticism of benefit cuts during the latter's administration.\n",
        "\n",
        "*Sentiment Analysis*\n",
        "\n",
        " Sentiment analysis revealed contrasts between administrations. During Adams’ tenure, positive sentiment appeared for \"benefits\" (0.71) and \"budget\" (0.25), while \"shelter\" (-0.51) reflected dissatisfaction. Sentiment for \"Adams\" (0.0003) was neutral, defying expectations of stronger opinions. It was surprising that the budget was not associated with more negative sentiment because the Adams administration has faced complaints from City Council and other government agencies about budget cuts. Shelter’s negative sentiment corresponds with my hypothesis that shelter would be viewed unfavorably during the Adams administration.\n",
        "\n",
        "De Blasio’s period showed more positive sentiment overall, with \"wage\" (0.77), \"budget\" (0.78), and \"benefits\" (0.77) scoring high.  Compared to the Adams timeframe, the majority of those times suggest that there was a more favorable perception around these themes. Budget was the only similarly high positive sentiment compared to Adams. Covid (0.15) and Wait (0.37) were of more mixed sentiment, suggesting a mix of views, but slightly more optimistic than during Adam’s tenure.\n",
        "\n",
        "Across most keywords, sentiment scores were significantly more positive during de Blasio’s tenure compared to Adam’s, especially for topics like Budget, Wage, and Housing. This corresponds with an increase in Covid-related programs to support wages and housing, expanding the budget as well. Discussions around Shelter were much more negative during Adams’ term, potentially indicating greater dissatisfaction with shelter policies. The Adams period showed more polarized sentiment, reflecting shifting public perceptions between the two administrations.\n",
        "\n",
        "A heatmap highlighted stark differences between administrations, with more polarized sentiment during Adams’ term. Word clouds showed \"Covid\" as dominant under de Blasio and \"wait\" and \"Adams\" prominent during Adams' tenure, aligning with heightened service delays and pandemic-era challenges.\n"
      ],
      "metadata": {
        "id": "nJk3Nu0-0pj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 NYC Open Data 311 Service Requests Results\n",
        "\n",
        "Among agencies, NYPD and HPD received the highest complaints, consistent with trends [(Council, 2024)](https://council.nyc.gov/data/311-agency/#:~:text=Top%20311%20Agency%20Responders,Department%20of%20Sanitation%20(DSNY)). Surprisingly, there was no significant increase in complaints related to welfare or human services under Adams, as hypothesized. \"Access\" was mentioned more under de Blasio, though still high across both periods.\n",
        "\n",
        "Keywords like \"housing,\" \"benefits,\" \"budget,\" \"migrant,\" and \"Covid\" had zero mentions in both administrations,  likely reflecting 311’s predefined complaint categories. There were some notable increases in complaints from de Blasio to Adams. “Shelter” increased by 22.5%, which could mirror the negative sentiment in Reddit, also indicating a potential growing concern or need for shelter-related services. “Wait” mentions rose by 55.2%, potentially highlighting the increase in benefit wait times during Adams term. “Immigration” complaints increased by 55.6%, likely indicating rising concerns about immigration services under Adams. \"Wage\" complaints also increased by 30%, potentially showing a heightened focus on wage-related issues. Conversely, complaints about \"employment\" dropped by 17.9%, and \"rent\" complaints declined by 8.7%. “Wage” had the highest number of mentions across both administrations, indicating that it is a significant issue for residents.\n",
        "\n",
        "Overall, shelter-related complaints rose significantly, mirroring negative Reddit sentiment, indicating increasing public concern for these services.\n"
      ],
      "metadata": {
        "id": "qOtXjhYd05j7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Discussion and Conclusion\n",
        "Several of my hypotheses were correct: during the de Blasio administration, frequent keywords in Reddit posts related to benefits, while under Adams, words like wait likely referred to delays in benefits enrollment. Sentiment around social services was notably more negative during the Adams administration, particularly regarding the term shelter, which may be tied to the rollback of the “Right to Shelter” policy. However, it is difficult to conclude a direct dependency on social services or to identify a significant difference in the volume of 311-related complaints between the two time periods.\n",
        "\n",
        "To expand upon this research, I would conduct a time series analysis for each keyword to observe how the frequency of mentions changes over time. Given the pandemic's potential influence, breaking mentions into monthly or weekly intervals could reveal clearer trends in public reactions to services. Additionally, combining Reddit and Open Data data sources to analyze frequency trends in parallel would provide a more comprehensive view.\n",
        "\n",
        "One key limitation of this research is the low volume of 311-related posts on Reddit, which may reflect the platform's demographics and its limited use for providing feedback on city services. Keywords in Reddit posts were more general and less frequent than anticipated, resulting in insufficient findings to identify policy gaps or inform decision-making.\n",
        "\n",
        "To address this, I plan to collect more data, refine the keyword search, and explore additional Reddit threads or alternative data sources. Instead of manually selecting keywords, I could use tools like ChatGPT or Claude to generate a list based on my research topic. I also aim to scrape data from subreddits focused on New York City social services, examining not only government-related threads but also discussions on how New Yorkers navigate unmet needs. Platforms like X (formerly Twitter), where 311 has a more active presence, could be valuable for this research, particularly if their APIs become accessible. Finally, expanding the scope to include other helplines, such as Cora, or nonprofit organizations could provide valuable insights into social service inquiries.\n",
        "\n",
        "There is a significant difference between the volume of 311 service requests and Reddit posts, likely due to how data is collected on each platform. On Reddit, users can freely post questions, share thoughts, or voice opinions, while 311 service requests are triaged. Callers may be redirected to other resources if their issue falls outside 311’s scope. For example, keywords like housing or homelessness may not appear if callers are referred to other agencies. Additionally, some agencies, like the Human Resources Administration (HRA), are not fully represented in the 311 dataset, further limiting insights into public perceptions.\n",
        "\n",
        "Furthermore, the populations that seek Reddit or 311 are quite different, which could also skew results. Reddit users are predominantly male, young, white, and well-educated [ (Barthel, Stocking, Holcomb, and Mitchell, 2016)](https://www.pewresearch.org/journalism/2016/02/25/reddit-news-users-more-likely-to-be-male-young-and-digital-in-their-news-preferences/), a demographic that does not typically rely on social services in New York. While this group may have opinions on major issues, their experiences likely differ from those of low-income or minority populations. Similarly, 311 users tend to skew toward older, white, well-educated females, another group less likely to access social services. Scraping data from additional platforms, such as Cora, could help address these discrepancies and provide a more representative view of social service inquiries.\n"
      ],
      "metadata": {
        "id": "4LB1A9hD1PAr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}