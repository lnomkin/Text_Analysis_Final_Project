{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lnomkin/Text_Analysis_Final_Project/blob/main/Copy_of_311_Service_Request_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#311 Service Request Comparison: NYC Open Data and NYC/311 Subreddit 2020-2023, from the De Blasio to the Adams administrations"
      ],
      "metadata": {
        "id": "0pjKPoEZPjHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Introduction: Understanding the public’s response to social service programming\n",
        "\n",
        "  The pandemic exposed New York City’s deep inequalities, impacting job loss, education access, and homelessness. Today, economic challenges and the end of Covid-19 aid continue to affect low-income communities. The city’s recovery efforts varied under de Blasio and Adams.\n",
        "  \n",
        "  De Blasio, elected on a progressive platform, saw a 15% wage increase for the bottom earners and a 13% decrease in poverty, the lowest since 2005 [Family, 2022](https://www.icph.org/reports/family-homelessness-in-new-york-city-what-the-adams-administration-can-learn-from-previous-mayoralties/#keeping-new-yorkers-housed-homelessness-prevention-vouchers-and-housing).  His key achievements included universal Pre-K and 3-K, easing childcare burdens, and streamlining social services. These gains reversed in the pandemic’s first year.\n",
        "  \n",
        "  Adams, elected on a moderate platform, focused on crime, economic recovery, and supporting small businesses [Get Stuff Done, 2024](https://www.nyc.gov/content/getstuffdone/pages/initiatives). Under his administration, cash-assistance recipients rose by 23%, while staffing shortages and an asylum crisis strained services [Family, 2022](https://www.icph.org/reports/family-homelessness-in-new-york-city-what-the-adams-administration-can-learn-from-previous-mayoralties/#keeping-new-yorkers-housed-homelessness-prevention-vouchers-and-housing). Poverty and unemployment remain high, and Adams faces the lowest approval ratings of any Mayor.\n",
        "  \n",
        "  Analyzing 311 service requests for social services during the pandemic offers insights into residents’ experiences. While Open Data NYC provides request types, the full text is unavailable. Complementing this with sentiment analysis of Reddit discussions can reveal how young adults and older residents perceived government services during this time, the majority of Reddit users [Anderson, 2024](https://www.socialchamp.io/blog/reddit-demographics/).\n",
        "\n",
        "  This research is critical for understanding how New Yorkers engage with social services. As the city faces ongoing challenges like the housing crisis, child care shortages, and rising welfare requests, this study can inform policymakers on public sentiment and service effectiveness by answering the following questions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PznULbPiP-Ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.2 Research Questions:\n",
        "1.  Is there a change in sentiment in NYC/311 Reddit posts across the de Blasio and Adams administrations, during a period of a shift in focus from self sufficiency to expanded benefits access?\n",
        "2.  Comparing key social service topics, what are the general trends in 311 service requests and NYC/311 Reddit posts related to social services across the two administrations? What is the sentiment of these themes?\n",
        "3.  What are the most frequently used keywords by the public on the r/NYC 311 subreddit and 311 service complaints?\n"
      ],
      "metadata": {
        "id": "qk16btDQR0S5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.3 Hypothesis:\n",
        "1.  There will likely be more complaints related to welfare and human services during the Adams administration due to expanded access leading to staffing shortages and delays in processing times.\n",
        "2.  The expanded pandemic-related aid created a dependency on social services, with complaints likely focusing on barriers to access, benefit expirations, and challenges in meeting basic needs.\n",
        "3.  Sentiment around social services during the Adams administration is expected to be more negative due to budget cuts, the city's response to the immigration crisis, and efforts to roll back the “Right to Shelter” policy.\n",
        "4.  Frequent keywords under de Blasio will include unemployment, benefits enrollment, and education, while under Adams, they are expected to focus on housing, migrants, and enrollment delays.\n"
      ],
      "metadata": {
        "id": "lOZd9vpCSF2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.4 Significance and Importance to Public Policy:\n",
        "This project offers insights into how New Yorkers respond to social services given mayoral changes. Policymakers could use these findings to assess what areas may need further resources or adjustments. Insights into the sentiment trends could inform perception of policy."
      ],
      "metadata": {
        "id": "j7lGuSPDSP4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0tQmVvrSSEi"
      },
      "outputs": [],
      "source": [
        "!pip install asyncpraw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gExL1gTDSttz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import asyncpraw\n",
        "import asyncio\n",
        "import time\n",
        "\n",
        "# Initialize Async PRAW\n",
        "reddit = asyncpraw.Reddit(\n",
        "    client_id=\"xb9MZu2WxExu2aLo94v8rA\",\n",
        "    client_secret=\"4s-xlQ0vw1qnK7Pa06o-tc8as4h0Yw\",\n",
        "    user_agent=\"reddit_text_extractor (by u/Good-Bread-994)\"\n",
        ")\n",
        "\n",
        "# Subreddits to fetch data from\n",
        "subreddits = [\"nyc\", \"AskNYC\", \"newyorkcity\", \"NYChousing\", \"311\"]\n",
        "\n",
        "# Function to fetch posts and their top parent comment\n",
        "async def fetch_posts(subreddits):\n",
        "    posts = []\n",
        "\n",
        "    # Loop through each subreddit\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = await reddit.subreddit(subreddit_name)  # Await the subreddit coroutine\n",
        "        async for submission in subreddit.top(limit=500):  # Adjust the limit if needed\n",
        "            # Ensure submission is fully loaded\n",
        "            await submission.load()\n",
        "\n",
        "            # Fetch the top parent comment\n",
        "            submission.comments.replace_more(limit=0)  # Replace \"more comments\" with the actual comments\n",
        "            top_comment = submission.comments[0].body if submission.comments else \"No comments\"\n",
        "\n",
        "            posts.append({\n",
        "                'subreddit': subreddit_name,\n",
        "                'title': submission.title,\n",
        "                'selftext': submission.selftext,\n",
        "                'created_utc': submission.created_utc,\n",
        "                'top_parent_comment': top_comment,  # Add the top parent comment\n",
        "            })\n",
        "\n",
        "    return posts\n",
        "\n",
        "# Function to run the async task and save to a CSV\n",
        "async def main():\n",
        "    posts = await fetch_posts(subreddits)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(posts)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv('reddit_threads_with_parent_comments.csv', index=False)\n",
        "    print(\"Data saved to 'reddit_threads_with_parent_comments.csv'\")\n",
        "\n",
        "    # Close the reddit session to avoid unclosed session warning\n",
        "    await reddit.close()\n",
        "\n",
        "# In Google Colab, use await directly:\n",
        "await main()\n",
        "#The following script is adapted from a Medium tutorial on building a Reddit Scraper with Python and Colab: https://python.plainenglish.io/two-step-wsb-scraper-with-colab-b240af5a6105 and from Melanie Walsh's Reddit Data Collection and Analysis: https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/14-Reddit-Data.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download file with Reddit data"
      ],
      "metadata": {
        "id": "k2xK7-jGfW28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqScPCWCxXmj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPwxdQFNW1Ce"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Load the existing CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/Python F24/Final Project/reddit_threads_with_parent_comments - Use for Project.csv')\n",
        "\n",
        "# Convert 'created_utc' to a datetime object (human-readable format)\n",
        "df['created_time'] = pd.to_datetime(df['created_utc'], unit='s')\n",
        "\n",
        "# Filter posts between 2020 and 2023\n",
        "reddit_df = df[(df['created_time'].dt.year >= 2020) & (df['created_time'].dt.year <= 2023)]\n",
        "#I adapted this script for the above and below codes from Rebecca Krisel's Intro to Pandas Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5_qf5XqXFpJ"
      },
      "outputs": [],
      "source": [
        "reddit_df.sample(100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqJNkrCvkuPP"
      },
      "outputs": [],
      "source": [
        "reddit_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4Klu95xnOye"
      },
      "outputs": [],
      "source": [
        "reddit_df['created_utc'] = pd.to_datetime(reddit_df['created_utc'],  unit='s')\n",
        "reddit_df['created_utc_str'] = reddit_df['created_utc'].dt.strftime('%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tWL5OmhnbrT"
      },
      "outputs": [],
      "source": [
        "reddit_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIx12CqnpMWt"
      },
      "outputs": [],
      "source": [
        "reddit_df[reddit_df.duplicated(keep=False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEEOVGhJpcDc"
      },
      "outputs": [],
      "source": [
        "reddit_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O52BAv_pfRN"
      },
      "outputs": [],
      "source": [
        "reddit_df=reddit_df.rename(columns={'created_utc_str':'date', 'selftext':'textpost'})\n",
        "reddit_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE0KjncAqgN5"
      },
      "outputs": [],
      "source": [
        "reddit_drop_date_df = reddit_df.drop(columns=['created_utc', 'created_time'])\n",
        "reddit_drop_date_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYv8ufbduHXB"
      },
      "outputs": [],
      "source": [
        "reddit_drop_date_df.sort_values(by='date', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiaTS2BnxDr6"
      },
      "outputs": [],
      "source": [
        "print(reddit_drop_date_df['date'].min(), reddit_drop_date_df['date'].max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_SSrX2VyEbL"
      },
      "outputs": [],
      "source": [
        "reddit_drop_date_df.groupby('subreddit').agg({'title': 'count', 'textpost': 'count', 'top_parent_comment': 'count', 'date': 'first'}).sort_values(by='title', ascending=False)"
      ]
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert 'date' to datetime format if it's not already\n",
        "reddit_drop_date_df['date'] = pd.to_datetime(reddit_drop_date_df['date'])\n",
        "\n",
        "# Group by week and month, counting the number of posts\n",
        "weekly_trend = reddit_drop_date_df.groupby(reddit_drop_date_df['date'].dt.to_period('W')).size()\n",
        "monthly_trend = reddit_drop_date_df.groupby(reddit_drop_date_df['date'].dt.to_period('M')).size()\n",
        "\n",
        "# Plotting weekly trend\n",
        "monthly_trend.plot(kind='line')\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Monthly Trend of Reddit Posts 2020-2023')  # Title of the plot\n",
        "plt.xlabel('Month')  # Label for the x-axis\n",
        "plt.ylabel('Number of Posts')  # Label for the y-axis\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()  # Adjust the layout for better readability\n",
        "plt.show()\n",
        "#I adapted the scripts to produce the monthly and annual trends from Rebecca Krisel's Intro to Pandas Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PYkjaJQCtfye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9Njif2hHiKU"
      },
      "outputs": [],
      "source": [
        "annual_trend = reddit_drop_date_df.groupby(reddit_drop_date_df['date'].dt.to_period('A')).size()\n",
        "\n",
        "annual_trend.plot(kind='line')\n",
        "\n",
        "plt.title('Annual Trend of Reddit Posts')  # Title of the plot\n",
        "plt.xlabel('Year')  # Label for the x-axis\n",
        "plt.ylabel('Number of Posts')  # Label for the y-axis\n",
        "\n",
        "plt.tight_layout()  # Adjust the layout for better readability\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TU-Y7Bp7GJn"
      },
      "outputs": [],
      "source": [
        "# Search for rows where any column contains the word\n",
        "result = reddit_drop_date_df.apply(lambda row: row.astype(str).str.contains('311', case=False).any(), axis=1)\n",
        "filtered_df = reddit_drop_date_df[result]\n",
        "\n",
        "total_count = len(filtered_df)\n",
        "\n",
        "# Print the total count\n",
        "print(\"Total count of rows containing '311':\", total_count)"
      ]
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab model\n",
        "\n",
        "\n",
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    if pd.isna(text):  # Check if the value is NaN\n",
        "        return ''  # Return an empty string for NaN values\n",
        "    tokens = word_tokenize(str(text))  # Tokenize the text\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalnum()]  # Remove stopwords and punctuation\n",
        "    return ' '.join(filtered_tokens)  # Join tokens back into a string\n",
        "\n",
        "# List of columns to clean\n",
        "columns_to_clean = ['top_parent_comment', 'title', 'textpost']  # Replace with your column names\n",
        "\n",
        "# Process each column\n",
        "for column in columns_to_clean:\n",
        "    if column in reddit_drop_date_df.columns:  # Ensure column exists in the DataFrame\n",
        "        reddit_drop_date_df[f'cleaned_{column}'] = reddit_drop_date_df[column].apply(tokenize_and_remove_stopwords)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(reddit_drop_date_df.head())\n",
        "# I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb and prompted ChatGPT to update the code to hanlde NaN values."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xzk0NboCjdd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VHEIizQ-QLC"
      },
      "outputs": [],
      "source": [
        "reddit_drop_date_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSN346kOMVXo"
      },
      "outputs": [],
      "source": [
        "reddit_short_df = reddit_drop_date_df.drop(columns=['title', 'textpost','top_parent_comment',])\n",
        "reddit_short_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLUITBZ0coZ0"
      },
      "outputs": [],
      "source": [
        "# Create a new column 'combined_text' that concatenates 'cleaned_title' and 'cleaned_textpost'\n",
        "reddit_short_df['combined_text'] = reddit_short_df['cleaned_title'] + ' ' + reddit_short_df['cleaned_textpost']\n",
        "\n",
        "# Display the updated DataFrame with the new 'combined_text' column\n",
        "print(reddit_short_df[['cleaned_title', 'cleaned_textpost', 'combined_text']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_short_df"
      ],
      "metadata": {
        "id": "k0NEF1bGbXKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2U0vx9Jcwpa"
      },
      "outputs": [],
      "source": [
        "# Drop the 'cleaned_textpost' column\n",
        "reddit_short_df = reddit_short_df.drop(columns=['cleaned_textpost', 'cleaned_title'])\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(reddit_short_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux-R8MX6dU6S"
      },
      "outputs": [],
      "source": [
        "reddit_short_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to separate by two time periods."
      ],
      "metadata": {
        "id": "tVeGuB0Ojt21"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import string\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Convert start_date and end_date to datetime64[ns]\n",
        "start_date_deblasio = pd.to_datetime('2020-01-01').to_numpy().astype('datetime64[ns]')\n",
        "end_date_deblasio = pd.to_datetime('2021-12-31').to_numpy().astype('datetime64[ns]')\n",
        "\n",
        "start_date_adams = pd.to_datetime('2022-01-01').to_numpy().astype('datetime64[ns]')\n",
        "end_date_adams = pd.to_datetime('2023-12-31').to_numpy().astype('datetime64[ns]')\n",
        "\n",
        "# Filter the DataFrame\n",
        "deblasio_df = reddit_short_df[(reddit_short_df['date'] >= start_date_deblasio) & (reddit_short_df['date'] <= end_date_deblasio)]\n",
        "adams_df = reddit_short_df[(reddit_short_df['date'] >= start_date_adams) & (reddit_short_df['date'] <= end_date_adams)]\n",
        "\n",
        "\n",
        "# Combine text columns into a single string\n",
        "text_columns = ['cleaned_top_parent_comment', 'combined_text']\n",
        "combined_text_deblasio = deblasio_df[text_columns].fillna('').agg(' '.join, axis=1).str.cat(sep=' ')\n",
        "combined_text_adams = adams_df[text_columns].fillna('').agg(' '.join, axis=1).str.cat(sep=' ')\n",
        "\n",
        "tokens_deblasio = word_tokenize(combined_text_deblasio)\n",
        "tokens_adams = word_tokenize(combined_text_adams)\n",
        "\n",
        "# Extend the stopword list\n",
        "custom_stop_words = {'nan', 'https', 'like', 'nyc', 'get', 'city', 'know', 'got', 'going', 'york', 'really', 'also', 'new', 'nan'}\n",
        "stop_words = set(stopwords.words('english')).union(custom_stop_words)\n",
        "\n",
        "# Clean the tokens\n",
        "tokens_deblasio = [word.lower() for word in tokens_deblasio if word.isalnum() and word.lower() not in stop_words]\n",
        "tokens_adams = [word.lower() for word in tokens_adams if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "# Generate frequency distribution\n",
        "word_counts_deblasio = Counter(tokens_deblasio)\n",
        "word_counts_adams = Counter(tokens_adams)\n",
        "\n",
        "# Get the top 20 keywords\n",
        "top_keywords_deblasio = word_counts_deblasio.most_common(20)\n",
        "top_keywords_adams = word_counts_adams.most_common(20)\n",
        "\n",
        "# Display the results\n",
        "print(\"Top 20 Keywords De blasio:\")\n",
        "for word1, count1 in top_keywords_deblasio:\n",
        "    print(f\"{word1}: {count1}\")\n",
        "\n",
        "print(\"Top 20 Keywords Adams:\")\n",
        "for word, count in top_keywords_adams:\n",
        "    print(f\"{word}: {count}\")\n",
        "#I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb and prompted ChatGPT to update the code with two timeframes."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5s5ncebxxuty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deblasio_df_key = pd.DataFrame(top_keywords_deblasio, columns=['Keyword', 'De Blasio']).set_index('Keyword')\n",
        "adams_df_key = pd.DataFrame(top_keywords_adams, columns=['Keyword', 'Adams']).set_index('Keyword')\n",
        "\n",
        "# Calculate the percentage difference\n",
        "comparison_df['Percentage Difference'] = (\n",
        "    (comparison_df['Adams'] - comparison_df['De Blasio']) / comparison_df['De Blasio']\n",
        ") * 100\n",
        "\n",
        "# Replace infinite or NaN values (from division by zero) with a placeholder like 0 or \"N/A\"\n",
        "comparison_df['Percentage Difference'] = comparison_df['Percentage Difference'].replace([float('inf'), -float('inf'), float('nan')], 0)\n",
        "\n",
        "# Sort by the percentage difference if desired\n",
        "comparison_df = comparison_df.sort_values(by='Percentage Difference', ascending=False)\n",
        "\n",
        "# Display the comparison table\n",
        "print(comparison_df)\n",
        "#I shared the frequency distribution with ChatGPT and asked it to calculate the percentage difference between the two time periods with a table."
      ],
      "metadata": {
        "id": "xYaUePnJkshE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deblasio_df"
      ],
      "metadata": {
        "id": "iESIAYU73a_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adams_df"
      ],
      "metadata": {
        "id": "y_pdReQw3T6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_text_adams"
      ],
      "metadata": {
        "id": "bnbVAsJv0esH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_text_deblasio"
      ],
      "metadata": {
        "id": "8ULNVBxW0hOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY9DBtNwVZpf"
      },
      "outputs": [],
      "source": [
        "key_words = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "print(key_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V39B51IXcNz"
      },
      "outputs": [],
      "source": [
        "keyword_frequencies_deblasio = {}\n",
        "keyword_frequencies_adams = {}\n",
        "for keyword in key_words:\n",
        "\n",
        "    keyword_frequencies_deblasio[keyword] = tokens_deblasio.count(keyword.lower())\n",
        "    keyword_frequencies_adams[keyword] = tokens_adams.count(keyword.lower())\n",
        "print(\"Keyword Frequencies de Blasio:\")\n",
        "for keyword, frequency in keyword_frequencies_deblasio.items():\n",
        "    print(f\"{keyword}: {frequency}\")\n",
        "print(\"Keyword Frequencies Adams:\")\n",
        "for keyword, frequency in keyword_frequencies_adams.items():\n",
        "    print(f\"{keyword}: {frequency}\")\n",
        "#I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjt4SpbmjA2m"
      },
      "outputs": [],
      "source": [
        "# Organize the keyword frequencies from smallest to largest\n",
        "sorted_frequencies_adams = sorted(keyword_frequencies_adams.items(), key=lambda x: x[1])\n",
        "sorted_frequencies_deblasio = sorted(keyword_frequencies_deblasio.items(), key=lambda x: x[1])\n",
        "\n",
        "print(\"Sorted Keyword Frequencies Adams (Smallest to Largest):\")\n",
        "for keyword, frequency in sorted_frequencies_adams:\n",
        "    print(f\"{keyword}: {frequency}\")\n",
        "print(\"Sorted Keyword Frequencies de Blasio (Smallest to Largest):\")\n",
        "for keyword, frequency in sorted_frequencies_deblasio:\n",
        "    print(f\"{keyword}: {frequency}\")\n",
        "#I prompted ChatGPT to organize the keyword frequencies from smallest to largest."
      ]
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define keywords and initialize frequency dictionaries\n",
        "key_words = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "# Initialize the keyword frequency dictionaries with 0 counts for each keyword\n",
        "keyword_frequencies_deblasio = {keyword: 0 for keyword in key_words}\n",
        "keyword_frequencies_adams = {keyword: 0 for keyword in key_words}\n",
        "\n",
        "for keyword in key_words:\n",
        "\n",
        "    keyword_frequencies_deblasio[keyword] = tokens_deblasio.count(keyword.lower())\n",
        "    keyword_frequencies_adams[keyword] = tokens_adams.count(keyword.lower())\n",
        "\n",
        "\n",
        "keywords = key_words\n",
        "de_blasio_frequencies = [keyword_frequencies_deblasio[keyword] for keyword in key_words]\n",
        "adams_frequencies = [keyword_frequencies_adams[keyword] for keyword in key_words]\n",
        "\n",
        "# Sort the frequencies\n",
        "sorted_frequencies_adams = dict(sorted(keyword_frequencies_adams.items(), key=lambda item: item[1], reverse=True))\n",
        "sorted_frequencies_deblasio = dict(sorted(keyword_frequencies_deblasio.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Plot the first chart for De Blasio\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(sorted_frequencies_deblasio.keys(), sorted_frequencies_deblasio.values(), color='skyblue')\n",
        "plt.xlabel('Keywords', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Frequency of Keywords in Cleaned Text - De Blasio', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot the second chart for Adams\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(sorted_frequencies_adams.keys(), sorted_frequencies_adams.values(), color='lightcoral')\n",
        "plt.xlabel('Keywords', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Frequency of Keywords in Cleaned Text - Adams', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "#I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb and asked ChatGPT to incorporate my keyword list"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jOmgeZEKplto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ShR3r11f1B5"
      },
      "source": [
        "Sentiment Analysis\n",
        "I used VADER, designed for short social media texts to capture subtle positive and negative tones in comments, to conduct the sentiment analysis on my keyword list. I downloaded the NLTK library for keyword extraction and co-occurrence analysis to reveal different trends in complaints across the shift from de Blasio to Adams. I segmented the posts by administration to capture if negativity increases around topics like housing shortages, covid, and benefits access delays."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('all')\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "#I adapted this script from Melanie Walsh's Sentiment Analysis: https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/04-Sentiment-Analysis.html"
      ],
      "metadata": {
        "id": "RQevBTqDv6Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(text):\n",
        "    # Get the sentiment scores for the text\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "\n",
        "    # Return the compound sentiment score (this score reflects the overall sentiment)\n",
        "    return scores['compound']\n",
        "\n",
        "# Ensure you're working with a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "adams_df = adams_df.copy()\n",
        "deblasio_df = deblasio_df.copy()\n",
        "\n",
        "# Apply the get_sentiment function using .loc to avoid SettingWithCopyWarning\n",
        "adams_df.loc[:, 'combined_text_sentiment'] = adams_df['combined_text'].apply(get_sentiment)\n",
        "adams_df.loc[:, 'parent_comment_sentiment'] = adams_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "deblasio_df.loc[:, 'combined_text_sentiment'] = deblasio_df['combined_text'].apply(get_sentiment)\n",
        "deblasio_df.loc[:, 'parent_comment_sentiment'] = deblasio_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "\n",
        "# Adjust display settings for horizontal output\n",
        "pd.set_option('display.max_colwidth', 100)  # Truncate long columns for better readability\n",
        "pd.set_option('display.expand_frame_repr', False)  # Prevent line wrapping for wide DataFrames\n",
        "\n",
        "# Adams Period Sentiment Analysis - Set index to comment ID for better readability\n",
        "adams_display = adams_df[['combined_text', 'combined_text_sentiment', 'cleaned_top_parent_comment', 'parent_comment_sentiment']]\n",
        "adams_display = adams_display.reset_index(drop=True)\n",
        "\n",
        "# De Blasio Period Sentiment Analysis - Set index to comment ID for better readability\n",
        "deblasio_display = deblasio_df[['combined_text', 'combined_text_sentiment', 'cleaned_top_parent_comment', 'parent_comment_sentiment']]\n",
        "deblasio_display = deblasio_display.reset_index(drop=True)\n",
        "\n",
        "# Print Results\n",
        "print(\"Adams Period Sentiment Analysis:\")\n",
        "print(adams_display)\n",
        "\n",
        "print(\"\\nDe Blasio Period Sentiment Analysis:\")\n",
        "print(deblasio_display)\n",
        "#Using Melanie Walsh's Sentiment Analysis guide, I prompted ChatGPT to write a code to apply the sentiment analysis to my two dataframes and specific columns"
      ],
      "metadata": {
        "id": "IF3xkCv-wX5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPwRHE_02ars"
      },
      "source": [
        "To analyze the average sentiment, I selected the key words from my initial hypthosis with a frequency of greater than 10. Anything under 10 was dropped, as the terms were less relevant than originally expected. The words I removed from my key word search were: immigration, migrant, delay, Blasio, and employment."
      ]
    },
    {
      "source": [
        "# Initialize the keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "# Function to calculate average sentiment for each keyword\n",
        "def calculate_average_sentiment(df, keywords, sentiment_column):\n",
        "    average_sentiments = {}\n",
        "\n",
        "    for keyword in keywords:\n",
        "        # Filter rows where the keyword is present in 'combined_text'\n",
        "        # using str.contains for case-insensitive search\n",
        "        keyword_rows = df[df['combined_text'].str.contains(keyword, case=False, na=False)]\n",
        "\n",
        "        # Calculate the average sentiment score for these rows\n",
        "        avg_sentiment = keyword_rows[sentiment_column].mean()\n",
        "        average_sentiments[keyword] = avg_sentiment\n",
        "\n",
        "    return average_sentiments\n",
        "\n",
        "# Calculate average sentiment for Adams period\n",
        "adams_average_sentiments = calculate_average_sentiment(adams_df, keywords, 'post_sentiment')\n",
        "\n",
        "# Calculate average sentiment for De Blasio period\n",
        "deblasio_average_sentiments = calculate_average_sentiment(deblasio_df, keywords, 'post_sentiment')\n",
        "\n",
        "# Print the average sentiment for each keyword for both periods\n",
        "print(\"Average Post Sentiment for Adams Period:\")\n",
        "for keyword, avg_sentiment in adams_average_sentiments.items():\n",
        "    print(f\"{keyword}: {avg_sentiment}\")\n",
        "\n",
        "print(\"\\nAverage Post Sentiment for De Blasio Period:\")\n",
        "for keyword, avg_sentiment in deblasio_average_sentiments.items():\n",
        "    print(f\"{keyword}: {avg_sentiment}\")\n",
        "\n",
        "#Using Melanie Walsh's Sentiment Analysis guide, I prompted ChatGPT to write a code to apply the sentiment analysis to my two dataframes and the post_sentiment column\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "716WuhnABSLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt  # Ensure you have plt imported for the plot\n",
        "\n",
        "# Define the keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "# Create a DataFrame\n",
        "# Convert dictionaries to lists for 'Adams Sentiment' and 'De Blasio Sentiment' columns\n",
        "data = {\n",
        "    'Keyword': keywords,\n",
        "    'Adams Sentiment': [adams_average_sentiments.get(keyword, float('nan')) for keyword in keywords],\n",
        "    'De Blasio Sentiment': [deblasio_average_sentiments.get(keyword, float('nan')) for keyword in keywords]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plotting heatmap\n",
        "df.set_index('Keyword', inplace=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.T, annot=True, cmap=\"coolwarm\", center=0, cbar_kws={'label': 'Sentiment'})\n",
        "plt.title('Sentiment Heatmap: Adams vs De Blasio')\n",
        "plt.show()\n",
        "\n",
        "#I adapted the above script from Geeks for Geeks Seaborn Heatmap Guide: https://www.geeksforgeeks.org/seaborn-heatmap-a-comprehensive-guide/ and I prompted ChatGPT to modify the code with my keyword list"
      ],
      "metadata": {
        "id": "dSRS7LnL1pFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUie86pZ5HEF"
      },
      "source": [
        "To analyze the sentiment of the keywords with a frequency distribution greater than 10, I took the average sentiment from the post sentiment column. Budget and Wage have the highest sentiment score. This likely alludes to the liminations of sentiment analysis - both of these keywords were viewed positively in the press or in anlysis of the mayors' administrations. Budget, particularly during the Adams administration was highly criticized as he cut the budget of all agencies by 5% and threated to significantly reduce library and senior services. Benefits had the highest sentiment score at 0.99, which could highlight the increase in benefits during the Covid pandemic or just show that Reddit users speak positively about benefits offered in NYC. Wage also had a high sentiment score under de Blasio, surprising given the percentage of people unemployed.\n",
        "\n",
        "Keywords that received the lowest sentiment scores were Adams at 0.004 and shelter at 0.13. The low rating of shelter from 2020 - 2023 aligns with my hypothesis that shelter would be perceived as negative during the timeframe. This is when shelters were at capacity due to the migrant crisis. Adams also has the lowest sentiment score."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure DataFrame slices are explicit copies to avoid SettingWithCopyWarning\n",
        "deblasio_df = reddit_short_df[(reddit_short_df['date'] >= start_date_deblasio) & (reddit_short_df['date'] <= end_date_deblasio)].copy()\n",
        "adams_df = reddit_short_df[(reddit_short_df['date'] >= start_date_adams) & (reddit_short_df['date'] <= end_date_adams)].copy()\n",
        "\n",
        "# Initialize the keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Create the get_sentiment function\n",
        "def get_sentiment(text):\n",
        "    # Get the sentiment scores for the text\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "\n",
        "    # Return the compound sentiment score (this score reflects the overall sentiment)\n",
        "    return scores['compound']\n",
        "\n",
        "# Apply the get_sentiment function to both DataFrames\n",
        "adams_df['parent_comment_sentiment'] = adams_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "deblasio_df['parent_comment_sentiment'] = deblasio_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "\n",
        "# Function to calculate average sentiment for each keyword for both periods\n",
        "def calculate_average_sentiment(df, keywords, sentiment_column):\n",
        "    average_sentiments = {}\n",
        "\n",
        "    for keyword in keywords:\n",
        "        # Filter rows where the keyword is present in 'combined_text'\n",
        "        # using str.contains for case-insensitive search\n",
        "        keyword_rows = df[df['combined_text'].str.contains(keyword, case=False, na=False)]\n",
        "\n",
        "        # Calculate the average sentiment score for these rows\n",
        "        avg_sentiment = keyword_rows[sentiment_column].mean()\n",
        "        average_sentiments[keyword] = avg_sentiment\n",
        "\n",
        "    return average_sentiments\n",
        "\n",
        "# Calculate average sentiment for Adams period\n",
        "adams_average_sentiments = calculate_average_sentiment(adams_df, keywords, 'parent_comment_sentiment')\n",
        "\n",
        "# Calculate average sentiment for De Blasio period\n",
        "deblasio_average_sentiments = calculate_average_sentiment(deblasio_df, keywords, 'parent_comment_sentiment')\n",
        "\n",
        "# Print the average sentiment for Adams period\n",
        "print(\"Average Parent Comment Sentiment for Adams Period:\")\n",
        "for keyword, sentiment in adams_average_sentiments.items():\n",
        "    print(f\"{keyword}: {sentiment}\")\n",
        "\n",
        "# Print a separator line\n",
        "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "\n",
        "# Print the average sentiment for De Blasio period\n",
        "print(\"Average Parent Comment Sentiment for De Blasio Period:\")\n",
        "for keyword, sentiment in deblasio_average_sentiments.items():\n",
        "    print(f\"{keyword}: {sentiment}\")\n",
        "\n",
        "#Using Melanie Walsh's Sentiment Analysis guide, I prompted ChatGPT to write a code to include the parent_comment sentiment for the Adams time period and de Blasio time period."
      ],
      "metadata": {
        "id": "aJTKtU_htmDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Define the keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "# Create a DataFrame\n",
        "# Convert dictionaries to lists for 'Adams Sentiment' and 'De Blasio Sentiment' columns\n",
        "data = {\n",
        "    'Keyword': keywords,\n",
        "    'Adams Sentiment': [adams_average_sentiments.get(keyword, float('nan')) for keyword in keywords],\n",
        "    'De Blasio Sentiment': [deblasio_average_sentiments.get(keyword, float('nan')) for keyword in keywords]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plotting heatmap\n",
        "df.set_index('Keyword', inplace=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.T, annot=True, cmap=\"coolwarm\", center=0, cbar_kws={'label': 'Sentiment'})\n",
        "plt.title('Sentiment Heatmap: Adams vs De Blasio')\n",
        "plt.show()\n",
        "\n",
        "#I adapted the above script from Geeks for Geeks Seaborn Heatmap Guide: https://www.geeksforgeeks.org/seaborn-heatmap-a-comprehensive-guide/ and I prompted ChatGPT to modify the code with my keyword list"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fyF537NR8vZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure DataFrame slices are explicit copies to avoid SettingWithCopyWarning\n",
        "deblasio_df = reddit_short_df[(reddit_short_df['date'] >= start_date_deblasio) & (reddit_short_df['date'] <= end_date_deblasio)].copy()\n",
        "adams_df = reddit_short_df[(reddit_short_df['date'] >= start_date_adams) & (reddit_short_df['date'] <= end_date_adams)].copy()\n",
        "\n",
        "# Initialize the keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Create the get_sentiment function\n",
        "def get_sentiment(text):\n",
        "    # Get the sentiment scores for the text\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "\n",
        "    # Return the compound sentiment score (this score reflects the overall sentiment)\n",
        "    return scores['compound']\n",
        "\n",
        "# Apply the get_sentiment function to both DataFrames, creating 'post_sentiment'\n",
        "adams_df['post_sentiment'] = adams_df['combined_text'].apply(get_sentiment) #Creating post_sentiment column\n",
        "adams_df['parent_comment_sentiment'] = adams_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "deblasio_df['post_sentiment'] = deblasio_df['combined_text'].apply(get_sentiment) #Creating post_sentiment column\n",
        "deblasio_df['parent_comment_sentiment'] = deblasio_df['cleaned_top_parent_comment'].apply(get_sentiment)\n",
        "\n",
        "# Function to calculate average sentiment for each keyword across both parent_comment_sentiment and post_sentiment\n",
        "def calculate_average_sentiment(df, keywords):\n",
        "    average_sentiments = {}\n",
        "\n",
        "    for keyword in keywords:\n",
        "        # Filter rows where the keyword is present in 'combined_text'\n",
        "        keyword_rows = df[df['combined_text'].str.contains(keyword, case=False, na=False)].copy()  # Explicit copy to avoid SettingWithCopyWarning\n",
        "\n",
        "        # Calculate the average sentiment score for these rows across both columns (parent_comment_sentiment and post_sentiment)\n",
        "        keyword_rows['average_sentiment'] = keyword_rows[['parent_comment_sentiment', 'post_sentiment']].mean(axis=1)\n",
        "\n",
        "        # Calculate the average of the combined sentiment for this keyword\n",
        "        avg_sentiment = keyword_rows['average_sentiment'].mean()\n",
        "        average_sentiments[keyword] = avg_sentiment\n",
        "\n",
        "    return average_sentiments\n",
        "\n",
        "# Calculate average sentiment for Adams period\n",
        "adams_average_sentiments = calculate_average_sentiment(adams_df, keywords)\n",
        "\n",
        "# Calculate average sentiment for De Blasio period\n",
        "deblasio_average_sentiments = calculate_average_sentiment(deblasio_df, keywords)\n",
        "\n",
        "# Print the average sentiment for each keyword for both periods\n",
        "print(\"Average Sentiment for Adams Period (combined):\")\n",
        "for keyword, avg_sentiment in adams_average_sentiments.items():\n",
        "    print(f\"{keyword}: {avg_sentiment}\")\n",
        "\n",
        "print(\"\\nAverage Sentiment for De Blasio Period (combined):\")\n",
        "for keyword, avg_sentiment in deblasio_average_sentiments.items():\n",
        "    print(f\"{keyword}: {avg_sentiment}\")\n",
        "\n",
        "#Using Melanie Walsh's Sentiment Analysis guide, I prompted ChatGPT to write a code to average the sentiment for the Adams time period and de Blasio time period."
      ],
      "metadata": {
        "id": "552bM_vi0oc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "# Create a DataFrame\n",
        "# Convert dictionaries to lists for 'Adams Sentiment' and 'De Blasio Sentiment' columns\n",
        "data = {\n",
        "    'Keyword': keywords,\n",
        "    'Adams Sentiment': [adams_average_sentiments.get(keyword, float('nan')) for keyword in keywords],\n",
        "    'De Blasio Sentiment': [deblasio_average_sentiments.get(keyword, float('nan')) for keyword in keywords]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Plotting heatmap\n",
        "df.set_index('Keyword', inplace=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.T, annot=True, cmap=\"coolwarm\", center=0, cbar_kws={'label': 'Sentiment'})\n",
        "plt.title('Sentiment Heatmap: Adams vs De Blasio')\n",
        "plt.show()\n",
        "\n",
        "#I adapted the above script from Geeks for Geeks Seaborn Heatmap Guide: https://www.geeksforgeeks.org/seaborn-heatmap-a-comprehensive-guide/ and I prompted ChatGPT to modify the code with my keyword list"
      ],
      "metadata": {
        "id": "w3S43l_Z091E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQerwZkjERo0"
      },
      "source": [
        "Adams has a slightly higher sentiment in parent comments but it is still the lowest of the key words. Benefits decreased significantly, by 0.4 to be 0.51. This could be because posts are generally more question oriented or neutral where as parent comments express a response to the post."
      ]
    },
    {
      "source": [
        "!pip install wordcloud matplotlib\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Keywords list\n",
        "keywords = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "# Function to calculate keyword frequencies for a given DataFrame\n",
        "def calculate_keyword_frequencies(df, keywords):\n",
        "    # Initialize an empty dictionary to store frequencies\n",
        "    keyword_frequencies = {}\n",
        "\n",
        "    # Loop through each keyword\n",
        "    for keyword in keywords:\n",
        "        # Count occurrences of the keyword in 'combined_text' column, ignoring case\n",
        "        keyword_frequencies[keyword] = df['combined_text'].str.contains(keyword, case=False).sum()\n",
        "\n",
        "    return keyword_frequencies\n",
        "\n",
        "# Calculate keyword frequencies for Adams period\n",
        "adams_keyword_frequencies = calculate_keyword_frequencies(adams_df, keywords)\n",
        "\n",
        "# Calculate keyword frequencies for De Blasio period\n",
        "deblasio_keyword_frequencies = calculate_keyword_frequencies(deblasio_df, keywords)\n",
        "\n",
        "# Create and display the word cloud for Adams period\n",
        "wordcloud_adams = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(adams_keyword_frequencies)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_adams, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off the axis\n",
        "plt.title(\"Adams Period - Keyword Frequency Word Cloud\")\n",
        "plt.show()\n",
        "\n",
        "# Create and display the word cloud for De Blasio period\n",
        "wordcloud_deblasio = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(deblasio_keyword_frequencies)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_deblasio, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off the axis\n",
        "plt.title(\"De Blasio Period - Keyword Frequency Word Cloud\")\n",
        "plt.show()\n",
        "\n",
        "#I adapted the above and below code from Geeks for Geeks Generating Word Cloud in Python Guide: https://www.geeksforgeeks.org/generating-word-cloud-python/ and asked ChatGPT to incorporate my key words and their respective frequency distribution"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vE5L76hX9GeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional keywords with their frequencies for both periods\n",
        "additional_keywords_adams = {\n",
        "    'people': 519, 'new': 429, 'one': 393, 'time': 312, 'would': 274, 'go': 266,\n",
        "    'back': 238, 'even': 215, 'see': 213, 'want': 210, 'day': 208, 'feel': 180,\n",
        "    'good': 176, '311': 176, 'work': 175, 'love': 171, 'every': 170, 'way': 168,\n",
        "    'years': 168, 'much': 167\n",
        "}\n",
        "\n",
        "additional_keywords_deblasio = {\n",
        "    'people': 489, 'new': 421, 'one': 389, 'time': 318, 'would': 268, 'go': 257,\n",
        "    'back': 230, 'even': 213, 'see': 210, 'want': 215, 'day': 206, 'feel': 173,\n",
        "    'good': 172, '311': 160, 'work': 160, 'love': 159, 'every': 155, 'way': 150,\n",
        "    'years': 162, 'much': 158\n",
        "}\n",
        "\n",
        "# Create and display the word cloud for Adams period using the additional keywords\n",
        "wordcloud_adams = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(additional_keywords_adams)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_adams, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off the axis\n",
        "plt.title(\"Adams Period - Additional Keyword Frequency Word Cloud\")\n",
        "plt.show()\n",
        "\n",
        "# Create and display the word cloud for De Blasio period using the additional keywords\n",
        "wordcloud_deblasio = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(additional_keywords_deblasio)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_deblasio, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off the axis\n",
        "plt.title(\"De Blasio Period - Additional Keyword Frequency Word Cloud\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eosmkk0YD9JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhR4WryA0DLh"
      },
      "source": [
        "NYC Open Data 311 Analysis\n",
        "1. df\n",
        "2. 2020 - 2023\n",
        "3. scrape 311 open data\n",
        "3. clean 311 open data: lemmatize, tokenize, remove stop words\n",
        "4. frequency analysis - broken up by de Blasio Jan 1, 2020 - Dec 31, 2021 and Adams Jan 1 2022 - Dec 31, 2023\n",
        "6. Word Cloud, frequency distribution matplob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Python F24/Final Project/311_Service_Requests (2).csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "#I adapted this code from Rebecca Krisel's Data Manipulation in Pandas and Python Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb"
      ],
      "metadata": {
        "id": "z1qWYttb5Qkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "jLdf2ke4zlLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['Location Type'])"
      ],
      "metadata": {
        "id": "RC2D74g6fINI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaSd4OagQEva"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "_ZF4DgEOGFhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Unique Key'] = df['Unique Key'].astype(str)\n",
        "\n",
        "print(df.dtypes)\n"
      ],
      "metadata": {
        "id": "3qZOhWAkWPYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Created Date'].min(), df['Created Date'].max())\n"
      ],
      "metadata": {
        "id": "Yma5zgE4qPC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "df['Created Date'] = pd.to_datetime(df['Created Date'],\n",
        "                                    format='%m/%d/%Y %I:%M:%S %p')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1r2BmK4yeXVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated(keep=False)]"
      ],
      "metadata": {
        "id": "FfWesFZvGNPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_agencies = df['Agency'].unique()\n",
        "\n",
        "print(unique_agencies)\n"
      ],
      "metadata": {
        "id": "XjydOhn7Gv2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I made the decision to include NYPD because calls about unhoused populations often are directed to the NYPD. The other agencies are directly related to benefits, wages, and social services."
      ],
      "metadata": {
        "id": "AtOOjZN3idWM"
      }
    },
    {
      "source": [
        "agencies_to_include = ['DOHMH', 'NYPD', 'HPD', 'DCWP', 'DHS']\n",
        "\n",
        "agencies_df = df[df['Agency'].isin(agencies_to_include)]\n",
        "\n",
        "print(agencies_df)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "s0_CVhLkW9Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am not going to tokenize because the data is already tokenized. Below I decided to remove any descriptor with more than one description because it just provides numbers and not words, not good for analysis."
      ],
      "metadata": {
        "id": "XLiSCM5ge4BA"
      }
    },
    {
      "source": [
        "df_filtered = agencies_df[agencies_df['Descriptor'].apply(lambda x: not bool(pd.notnull(x) and isinstance(x, str) and any(char.isdigit() for char in x)))]\n",
        "\n",
        "print(df_filtered)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lrklxzsvgPp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered"
      ],
      "metadata": {
        "id": "kW0fTdt4pRB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to divide my data into two date ranges per administration and summarize the number of complaints."
      ],
      "metadata": {
        "id": "dNoDljwAhci3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date_range1_start = '2020-01-01'\n",
        "date_range1_end = '2021-12-31'\n",
        "date_range2_start = '2022-01-01'\n",
        "date_range2_end = '2023-12-31'\n",
        "\n",
        "# Filter the DataFrame for each date range\n",
        "df_range1 = df_filtered[(df_filtered['Created Date'] >= date_range1_start) & (df_filtered['Created Date'] <= date_range1_end)]\n",
        "df_range2 = df_filtered[(df_filtered['Created Date'] >= date_range2_start) & (df_filtered['Created Date'] <= date_range2_end)]\n",
        "\n",
        "# Group by 'Agency' and count the complaints\n",
        "complaints_range1 = df_range1['Agency'].value_counts().reset_index()\n",
        "complaints_range1.columns = ['Agency', 'Complaints (2020-2021)']\n",
        "\n",
        "complaints_range2 = df_range2['Agency'].value_counts().reset_index()\n",
        "complaints_range2.columns = ['Agency', 'Complaints (2022-2023)']\n",
        "\n",
        "# Merge the results into a single DataFrame\n",
        "complaints_summary = pd.merge(\n",
        "    complaints_range1,\n",
        "    complaints_range2,\n",
        "    on='Agency',\n",
        "    how='outer'\n",
        ").fillna(0)  # Fill NaN values with 0\n",
        "\n",
        "# Convert counts to integers\n",
        "complaints_summary[['Complaints (2020-2021)', 'Complaints (2022-2023)']] = complaints_summary[\n",
        "    ['Complaints (2020-2021)', 'Complaints (2022-2023)']\n",
        "].astype(int)\n",
        "\n",
        "# Display the summary\n",
        "print(complaints_summary)\n",
        "\n",
        "#I prompted ChatGPT to group the complaints by agency across the two timeframes and print as a table."
      ],
      "metadata": {
        "id": "JqczoFFcpqwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set positions and width for the bars\n",
        "x = np.arange(len(complaints_summary['Agency']))  # label locations\n",
        "width = 0.35  # width of bars\n",
        "\n",
        "# Create the figure and axes\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Bar plots for each date range\n",
        "bars1 = ax.bar(x - width/2, complaints_summary['Complaints (2020-2021)'], width, label='2020-2021')\n",
        "bars2 = ax.bar(x + width/2, complaints_summary['Complaints (2022-2023)'], width, label='2022-2023')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Agency', fontsize=12)\n",
        "ax.set_ylabel('Number of Complaints', fontsize=12)\n",
        "ax.set_title('Number of Complaints by Agency (2020-2023)', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(complaints_summary['Agency'], rotation=45, ha='right')\n",
        "ax.legend()\n",
        "\n",
        "# Add value annotations on top of bars\n",
        "for bar in bars1 + bars2:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height}',\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3),  # Offset text above the bar\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "# Improve layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#I adapted this code from Rebecca Krisel's Data Manipulation in Pandas and Python Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb and prompted ChatGPT to modify the code with the two time frames"
      ],
      "metadata": {
        "id": "ffXeIvAqjPuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "\n",
        "# Ensure stopwords are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab model\n",
        "\n",
        "# Define stop words and punctuation\n",
        "custom_stop_words = ['nan']\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Function to preprocess and get word frequencies\n",
        "def get_top_words(df, column_name, top_n=20):\n",
        "    # Combine all text in the specified column\n",
        "    all_text = ' '.join(df[column_name].dropna().astype(str))\n",
        "\n",
        "    # Tokenize, remove punctuation and stop words, and convert to lowercase\n",
        "    tokens = word_tokenize(all_text)\n",
        "    cleaned_tokens = [\n",
        "        word.lower() for word in tokens\n",
        "        if word.lower() not in stop_words and word not in punctuation and word.isalpha()\n",
        "    ]\n",
        "\n",
        "    # Get word frequencies\n",
        "    freq_dist = FreqDist(cleaned_tokens)\n",
        "    return freq_dist.most_common(top_n)\n",
        "\n",
        "# Get top 20 words for each period\n",
        "df_filtered.loc[:, 'Created Date'] = pd.to_datetime(df_filtered['Created Date'])\n",
        "\n",
        "df_range1 = df_filtered.loc[(df_filtered['Created Date'] >= date_range1_start) & (df_filtered['Created Date'] <= date_range1_end)]\n",
        "df_range2 = df_filtered.loc[(df_filtered['Created Date'] >= date_range2_start) & (df_filtered['Created Date'] <= date_range2_end)]\n",
        "\n",
        "top_words_range1 = get_top_words(df_range1, 'Descriptor', top_n=20)\n",
        "top_words_range2 = get_top_words(df_range2, 'Descriptor', top_n=20)\n",
        "\n",
        "# Print results\n",
        "print(\"Top 20 Words for de Blasio Administration in 311 Data:\")\n",
        "for word, count in top_words_range1:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nTop 20 Words for Adams Administration in 311 Data:\")\n",
        "for word, count in top_words_range2:\n",
        "    print(f\"{word}: {count}\")\n",
        "#I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb and prompted ChatGPT to update the code with two timeframes, custom stop words, and top 20 words."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KBx2b1b0keYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "range1_df = range1_df.sort_values(by='Count', ascending=False)\n",
        "range2_df = range2_df.sort_values(by='Count', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range1_df['Word'], range1_df['Count'], color='skyblue')\n",
        "plt.xlabel('Keywords', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Top Keywords in 311 Data During de Blasio Administration', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range2_df['Word'], range2_df['Count'], color='lightcoral')\n",
        "plt.xlabel('Keywords', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Top Keywords in 311 Data During Adams Administration', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#I adapted this script from Rebecca Krisel's Pandas Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb"
      ],
      "metadata": {
        "id": "-wnXiXB5Mk7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_words = ['housing', 'benefits', 'shelter', 'wait', 'delays', 'budget', 'immigration', 'migrant', 'covid', 'blasio', 'adams', 'wage', 'employment', 'rent']\n",
        "\n",
        "def count_keywords(df, key_words, text_column):\n",
        "    keyword_counts = {word: 0 for word in key_words}  # Initialize dictionary with 0 counts for each keyword\n",
        "\n",
        "    # Iterate over each row in the DataFrame\n",
        "    for text in df[text_column].dropna():  # Use dropna() to skip NaN values\n",
        "        # Convert text to lowercase for case-insensitive matching\n",
        "        text = text.lower()\n",
        "\n",
        "        # Check each keyword and count its occurrences\n",
        "        for word in key_words:\n",
        "            keyword_counts[word] += text.count(word)  # Count occurrences of the word in the text\n",
        "\n",
        "    return keyword_counts\n",
        "\n",
        "keyword_counts_deblasio = count_keywords(df_range1, key_words, 'Descriptor')\n",
        "keyword_counts_adams = count_keywords(df_range2, key_words, 'Descriptor')\n",
        "\n",
        "print(\"Keyword Counts for de Blasio Administration:\")\n",
        "for word, count in keyword_counts_deblasio.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nKeyword Counts for Adams Administration:\")\n",
        "for word, count in keyword_counts_adams.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "#I adapted this script from Rebecca Krisel's Intro to NLTK Workshop: https://github.com/rskrisel/intro_to_nltk/blob/main/Intro_NLTK_workshop.ipynb and prompted ChatGPT to update the code with two timeframes and my keyword list"
      ],
      "metadata": {
        "id": "oS4v8r23pD5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_deblasio_counts = sorted(keyword_counts_deblasio.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_adams_counts = sorted(keyword_counts_adams.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "deblasio_keywords, deblasio_counts = zip(*sorted_deblasio_counts)\n",
        "adams_keywords, adams_counts = zip(*sorted_adams_counts)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(deblasio_keywords, deblasio_counts, color='skyblue')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Keyword Counts for de Blasio Administration\", fontsize=14)\n",
        "plt.xlabel(\"Keywords\", fontsize=12)\n",
        "plt.ylabel(\"Counts\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(adams_keywords, adams_counts, color='lightcoral')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Keyword Counts for Adams Administration\", fontsize=14)\n",
        "plt.xlabel(\"Keywords\", fontsize=12)\n",
        "plt.ylabel(\"Counts\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#I adapted this script from Rebecca Krisel's Pandas Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb"
      ],
      "metadata": {
        "id": "l-Qr_tvsM9ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Convert 'Created Date' to datetime format if it's not already\n",
        "df_filtered['Created Date'] = pd.to_datetime(df_filtered['Created Date'])\n",
        "\n",
        "# Group by month, counting the number of complaints\n",
        "monthly_trend = df_filtered.groupby(df_filtered['Created Date'].dt.to_period('M')).size()\n",
        "\n",
        "# Set up the figure size for a larger plot\n",
        "plt.figure(figsize=(12, 6))  # Increase the width and height for a larger graph\n",
        "\n",
        "# Plotting the monthly trend\n",
        "monthly_trend.plot(kind='line', color='b', linewidth=2)  # Use smooth lines with consistent color (blue)\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Monthly Trend of Open Data 311 Complaints', fontsize=16)  # Title with larger font size\n",
        "plt.xlabel('Month', fontsize=14)  # Label for the x-axis with larger font size\n",
        "plt.ylabel('Number of Complaints', fontsize=14)  # Label for the y-axis with larger font size\n",
        "\n",
        "# Adjust layout for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plotting complaints per year\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(complaints_per_year['Year'], complaints_per_year['Complaints'], color='green')\n",
        "plt.title('311 Service Complaints Per Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Complaints')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()  # Adjust layout for readability\n",
        "plt.show()\n",
        "\n",
        "#I adapted this script from Rebecca Krisel's Pandas Workshop: https://github.com/rskrisel/pandas/blob/main/pandas_workshop_2024.ipynb and prompted ChatGPT to adjust the layout for readability and to include the monthly trend as a line graph and the annual trend as a bar graph."
      ],
      "metadata": {
        "id": "swJU5IU-W5KM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}